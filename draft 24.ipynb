{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9475275,"sourceType":"datasetVersion","datasetId":5762643},{"sourceId":9475855,"sourceType":"datasetVersion","datasetId":5763086},{"sourceId":9852663,"sourceType":"datasetVersion","datasetId":6045841},{"sourceId":9852677,"sourceType":"datasetVersion","datasetId":6045855},{"sourceId":9868970,"sourceType":"datasetVersion","datasetId":6057929},{"sourceId":9869541,"sourceType":"datasetVersion","datasetId":6058354},{"sourceId":9880613,"sourceType":"datasetVersion","datasetId":6066629},{"sourceId":9881095,"sourceType":"datasetVersion","datasetId":6067037},{"sourceId":9902121,"sourceType":"datasetVersion","datasetId":6082955}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### main to load data but is exausting ram","metadata":{}},{"cell_type":"markdown","source":"load full 100% train data","metadata":{}},{"cell_type":"markdown","source":"## 1. Loading visual features\n ","metadata":{}},{"cell_type":"code","source":"# !pip install torchfile --quiet\n# import os\n# import numpy as np\n# import torchfile\n\n# # Base directory of your dataset\n# BASE_DIR = \"/kaggle/input/visual/feat_dat\"\n\n# # Define directories for train, validation, and test\n# train_dir = os.path.join(BASE_DIR, \"train_frame_feat\")\n# # val_dir = os.path.join(BASE_DIR, \"val_frame_feat\")\n# # test_dir = os.path.join(BASE_DIR, \"test_frame_feat\")\n\n# # Function to load .dat feature files\n# def load_features(feature_path):\n#     try:\n#         # Load Lua Torch file\n#         features = torchfile.load(feature_path)\n#         return features  # The file content is likely an array or tensor\n#     except Exception as e:\n#         print(f\"Error loading file {feature_path}: {e}\")\n#         return None\n\n# # Function to iterate over dataset directories\n# def load_dataset_features(base_dir):\n#     dataset_features = {}\n#     for root, _, files in os.walk(base_dir):\n#         for file in files:\n#             if file.endswith(\".dat\"):\n#                 video_id = os.path.relpath(root, base_dir)\n#                 feature_path = os.path.join(root, file)\n#                 # print(f\"Loading features for video: {video_id}\")\n#                 features = load_features(feature_path)\n#                 if features is not None:\n#                     dataset_features[video_id] = features\n#     return dataset_features\n\n# # Load train, validation, and test features\n# print(\"Loading training features...\")\n# train_features = load_dataset_features(train_dir)\n\n# # print(\"Loading validation features...\")\n# # val_features = load_dataset_features(val_dir)\n\n# # print(\"Loading test features...\")\n# # test_features = load_dataset_features(test_dir)\n\n# # Example: Accessing a sample video feature\n# sample_video_id = list(train_features.keys())[0]\n# print(f\"Sample Video ID: {sample_video_id}\")\n# print(f\"Feature Shape: {train_features[sample_video_id].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### load 50 % of the train data and 100-100 % of val and test data below with code above","metadata":{}},{"cell_type":"code","source":"!pip install torchfile --quiet\nimport os\nimport numpy as np\nimport torchfile\nimport random\n\n# Function to load .dat feature files\ndef load_features(feature_path):\n    try:\n        features = torchfile.load(feature_path)\n        return features\n    except Exception as e:\n        print(f\"Error loading file {feature_path}: {e}\")\n        return None\n\n# Function to load a fraction of dataset features\n#to load 10 % make fraction=0.1\ndef load_dataset_features_partial(base_dir, fraction=0.1, seed=42):\n    dataset_features = {}\n    random.seed(seed)  # Ensure reproducibility\n\n    # Collect all .dat files\n    all_files = []\n    for root, _, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(\".dat\"):\n                video_id = os.path.relpath(root, base_dir)\n                feature_path = os.path.join(root, file)\n                all_files.append((video_id, feature_path))\n    \n    # Shuffle and select a fraction of the files\n    random.shuffle(all_files)\n    subset_files = all_files[:int(len(all_files) * fraction)]\n\n    # Load the selected files\n    for video_id, feature_path in subset_files:\n        features = load_features(feature_path)\n        if features is not None:\n            dataset_features[video_id] = features\n    \n    return dataset_features\n\n# Base directories\nBASE_DIR = \"/kaggle/input/visual/feat_dat\"\ntrain_dir = os.path.join(BASE_DIR, \"train_frame_feat\")\nval_dir = os.path.join(BASE_DIR, \"val_frame_feat\")\ntest_dir = os.path.join(BASE_DIR, \"test_frame_feat\")\n\n\nfraction=0.1\n# Load only 30% of the dataset\nprint(f\"Loading {fraction * 100}% of training features...\")\n# train_features_partial = load_dataset_features_partial(train_dir, fraction=0.1)\ntrain_features = load_dataset_features_partial(train_dir, fraction)\n\n\nprint(f\"Loading 100% of validation features...\")\n# val_features_partial = load_dataset_features_partial(val_dir, fraction=0.1)\nval_features = load_dataset_features_partial(val_dir, 1)\n\nprint(f\"Loading 100% of Test features...\")\n# val_features_partial = load_dataset_features_partial(val_dir, fraction=0.1)\ntest_features = load_dataset_features_partial(test_dir, 1)\n\n\n\n# Check the number of loaded features\nprint(f\"Train Features Loaded: {len(train_features)}\")\nprint(f\"Validation Features Loaded: {len(val_features)}\")\nprint(f\"Test Features Loaded: {len(test_features)}\")\n\n\n# Example: Inspect a loaded feature\nif train_features:\n    sample_video_id = list(train_features.keys())[0]\n    print(f\"Sample Video ID: {sample_video_id}\")\n    print(f\"Feature Shape: {train_features[sample_video_id].shape}\")\n    \nif test_features:\n    sample_video_id_test = list(test_features.keys())[0]\n    print(f\"Sample Video ID: {sample_video_id_test}\")\n    print(f\"Feature Shape: {test_features[sample_video_id_test].shape}\")\n    \nif val_features:\n    sample_video_id_val = list(val_features.keys())[0]\n    print(f\"Sample Video ID: {sample_video_id_val}\")\n    print(f\"Feature Shape: {val_features[sample_video_id_val].shape}\")    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Path to save the features\nsave_path = \"test.npz\"\n\n# Save the dictionary as a compressed NumPy file\nnp.savez_compressed(save_path, test_features)\nprint(f\"Features saved to {save_path}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Path to the saved file\n# load_path = \"train_features.npz\"\n\n# # Load the dictionary from the compressed file\n# loaded_features = np.load(load_path, allow_pickle=True)\n\n# # Convert back to dictionary\n# train_features = {key: loaded_features[key] for key in loaded_features.files}\n# print(f\"Loaded {len(train_features)} features from {load_path}.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save the transformed features\n# np.save(\"transformed_train_features.npy\", transformed_train_features)\nnp.save(\"transformed_val_features.npz\", test_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"above is of shape Feature Shape: (500, 512) , we are converting it into shape 2048 dimensional vector\r\n","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import torch\n# import torch.nn as nn\n\n# # Define the linear transformation\n# linear_proj = nn.Linear(512, 2048)\n\n# # Function to transform features\n# def transform_features(features_dict, linear_layer):\n#     transformed_features = {}\n#     for video_id, features in features_dict.items():\n#         features_tensor = torch.tensor(features, dtype=torch.float32)\n#         projected = linear_layer(features_tensor)\n#         transformed_features[video_id] = projected.detach().numpy()\n#     return transformed_features\n\n# # Apply transformation to train ,test and validation features\n# print(\"Transforming training features...\")\n# # transformed_train_features = transform_features(train_features, linear_proj)\n# train_features = transform_features(train_features, linear_proj)\n\n# print(\"Transforming validation features...\")\n# val_features = transform_features(val_features, linear_proj)\n\n# print(\"Transforming Test features...\")\n# test_features = transform_features(test_features, linear_proj)\n\n\n\n# # # Save the transformed features\n# # np.save(\"transformed_train_features.npy\", transformed_train_features)\n# # np.save(\"transformed_val_features.npy\", transformed_val_features)\n\n\n# print(f\"Transformed training feature shape: {train_features[sample_video_id].shape}\")\n# print(f\"Transformed training feature shape: {val_features[sample_video_id_val].shape}\")\n# print(f\"Transformed training feature shape: {test_features[sample_video_id_test].shape}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"same code as above + extra feature of :\nNormalize train_features Keys: Extract the actual video IDs from the complex paths in train_features keys. For example:\r\n\r\n'224/YUWG15N2yw4/0010' â†’ 'YUWG15N2yw4' Code for Normalization: Use the following code to normalize train_features key\n\nto reduce ram usages:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\n\n# Define the linear transformation\nlinear_proj = nn.Linear(512, 2048)\n\n# Function to normalize keys and transform features\ndef normalize_and_transform_features(features_dict, linear_layer):\n    transformed_features = {}\n    for key, features in features_dict.items():\n        # Normalize the key to extract the video ID\n        video_id = key.split('/')[1]  # Adjust split index based on your key format\n        # Convert features to tensor and apply transformation\n        features_tensor = torch.tensor(features, dtype=torch.float32)\n        projected = linear_layer(features_tensor)\n        # Store transformed features with normalized video ID as key\n        transformed_features[video_id] = projected.detach().numpy()\n    return transformed_features\n\n# Apply normalization and transformation\nprint(\"Processing training features...\")\ntrain_features = normalize_and_transform_features(train_features, linear_proj)\n\nprint(\"Processing validation features...\")\nval_features = normalize_and_transform_features(val_features, linear_proj)\n\nprint(\"Processing test features...\")\ntest_features = normalize_and_transform_features(test_features, linear_proj)\n\n# Debug the transformed and normalized keys\nprint(f\"Sample normalized training feature keys: {list(train_features.keys())[:5]}\")\nprint(f\"Sample normalized validation feature keys: {list(val_features.keys())[:5]}\")\nprint(f\"Sample normalized test feature keys: {list(test_features.keys())[:5]}\")\n\n# Debug transformed feature shapes\nsample_video_id_train = list(train_features.keys())[0]\nsample_video_id_val = list(val_features.keys())[0]\nsample_video_id_test = list(test_features.keys())[0]\n\nprint(f\"Transformed training feature shape: {train_features[sample_video_id_train].shape}\")\nprint(f\"Transformed validation feature shape: {val_features[sample_video_id_val].shape}\")\nprint(f\"Transformed test feature shape: {test_features[sample_video_id_test].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2. Check the total number of videos loaded  ","metadata":{}},{"cell_type":"code","source":"# Check the total number of videos loaded\nprint(f\"Total train videos loaded: {len(train_features)}\")\nprint(f\"Total validation videos loaded: {len(val_features)}\")\nprint(f\"Total test videos loaded: {len(test_features)}\")\n\n# Inspect a sample video's of train,val,test feature \nsample_video_id = list(train_features.keys())[0]  # Get the first video ID\nsample_features = train_features[sample_video_id]\n\nprint(f\"Sample Video ID: {sample_video_id}\")\nprint(f\"Feature Type: {type(sample_features)}\")\nprint(f\"Feature Shape: {np.array(sample_features).shape}\")  # Convert to NumPy array if needed\n\n\n\nsample_video_id_val = list(val_features.keys())[0]  # Get the first video ID\nsample_features_val = val_features[sample_video_id_val]\n\nprint(f\"Sample Video ID: {sample_video_id_val}\")\nprint(f\"Feature Type: {type(sample_features_val)}\")\nprint(f\"Feature Shape: {np.array(sample_features_val).shape}\")  # Convert to NumPy array if needed\n\n\n\nsample_video_id_test = list(test_features.keys())[0]  # Get the first video ID\nsample_features_test = test_features[sample_video_id_test]\n\nprint(f\"Sample Video ID: {sample_video_id_test}\")\nprint(f\"Feature Type: {type(sample_features_test)}\")\nprint(f\"Feature Shape: {np.array(sample_features_test).shape}\")  # Convert to NumPy array if needed\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print a subset (e.g., the first 10 elements) of the feature values for inspection\nprint(f\"First 10 values of the train feature for Video ID {sample_video_id}:\")\nprint(sample_features[:10])  # Adjust the slice as needed\n\nprint(f\"First 10 values of the val feature for Video ID {sample_video_id_val}:\")\nprint(sample_features_val[:10])  # Adjust the slice as needed\n\nprint(f\"First 10 values of the test feature for Video ID {sample_video_id_test}:\")\nprint(sample_features_test[:10])  # Adjust the slice as needed\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.3. Statistics of a sample feature ","metadata":{}},{"cell_type":"code","source":"# Statistics of a train sample feature\nprint(f\"Feature Min Value: {np.min(sample_features)}\")\nprint(f\"Feature Max Value: {np.max(sample_features)}\")\nprint(f\"Feature Mean Value: {np.mean(sample_features)}\")\nprint(f\"Feature Standard Deviation: {np.std(sample_features)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.4. Visualize the histogram of feature values","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Visualize the histogram of train , val, test feature values\nplt.hist(np.ravel(sample_features), bins=50, color='blue', alpha=0.7)\nplt.title(\"Histogram of Feature Values\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.hist(np.ravel(sample_features_val), bins=50, color='blue', alpha=0.7)\nplt.title(\"Histogram of Feature Values\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.hist(np.ravel(sample_features_test), bins=50, color='blue', alpha=0.7)\nplt.title(\"Histogram of Feature Values\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.5.Check shapes of multiple video features","metadata":{}},{"cell_type":"code","source":"# Check shapes of multiple train video features\nfor i, (video_id, features) in enumerate(train_features.items()):\n    print(f\"Video ID: {video_id}, Feature Shape: {np.array(features).shape}\")\n    if i >= 4:  # Limit to the first 5 videos\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Expected shape of features (e.g., for ResNet-34 it may be 2048 dimensions)\n# expected_shape = (2048,)\n\n# # Verify shapes for all videos in the train set\n# for video_id, features in train_features.items():\n#     if np.array(features).shape[-1] != expected_shape[0]:\n#         print(f\"Mismatch in feature shape for Video ID: {video_id}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"##  2. Load the Annotations","metadata":{}},{"cell_type":"code","source":"# import json\n\n# # Replace 'path_to_ground_truth.json' with your actual file path\n# ground_truth_path = \"path_to_ground_truth.json\"\n# with open(ground_truth_path, \"r\") as f:\n#     ground_truthannotations = json.load(f)  # Load JSON into a Python dictionary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\n\ndef parse_annotations(annotation_path):\n    # Load JSON file\n    with open(annotation_path, 'r') as f:\n        data = json.load(f)\n    \n    parsed_data = []\n    database = data.get(\"database\", {})\n    \n    for video_id, video_info in database.items():\n        subset = video_info.get(\"subset\", \"\")\n        duration = video_info.get(\"duration\", 0)\n        annotations = video_info.get(\"annotations\", [])\n        video_url = video_info.get(\"video_url\", \"\")\n\n        for annotation in annotations:\n            parsed_data.append({\n                \"video_id\": video_id,\n                \"subset\": subset,\n                \"duration\": duration,\n                \"start_time\": annotation[\"segment\"][0],\n                \"end_time\": annotation[\"segment\"][1],\n                \"caption\": annotation.get(\"sentence\", \"\"),\n                \"video_url\": video_url\n            })\n    return parsed_data\n\n# Paths to annotation files\ntrainval_path = \"/kaggle/input/annotations/youcookii_annotations_trainval/youcookii_annotations_trainval.json\"\ntest_path = \"/kaggle/input/annotations/youcookii_annotations_test_segments_only/youcookii_annotations_test_segments_only.json\"\n\n# Parse data\ntrainval_data = parse_annotations(trainval_path)\ntest_data = parse_annotations(test_path)\n\n# Check parsed data\nprint(f\"Training/Validation Data: {len(trainval_data)} entries\")\nprint(f\"Testing Data: {len(test_data)} entries\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n\n# def align_features_with_segments(parsed_data, visual_features, feature_duration):\n#     aligned_data = []\n    \n#     for item in parsed_data:\n#         video_id = item[\"video_id\"]\n#         start_idx = int(item[\"start_time\"] / feature_duration)\n#         end_idx = int(item[\"end_time\"] / feature_duration)\n        \n#         # Ensure indices are within bounds\n#         start_idx = max(0, start_idx)\n#         end_idx = min(len(visual_features), end_idx)\n\n#         aligned_data.append({\n#             \"video_id\": video_id,\n#             \"features\": visual_features[start_idx:end_idx],\n#             \"caption\": item[\"caption\"],\n#             \"start_time\": item[\"start_time\"],\n#             \"end_time\": item[\"end_time\"],\n#             \"subset\": item[\"subset\"]\n#         })\n#     return aligned_data\n\n# # Example: Align with dummy features\n# dummy_visual_features = np.random.rand(500, 512)  # Replace with actual features\n# feature_duration = 1.0  # Each feature corresponds to 1 second\n    \n# aligned_trainval = align_features_with_segments(trainval_data, dummy_visual_features, feature_duration)\n# print(f\"Aligned Training/Validation Data: {len(aligned_trainval)} segments\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"## 3 Linguistic Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Extract N-Grams from Captions","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom nltk.util import ngrams\nimport nltk\n\nnltk.download('punkt')\n\ndef extract_ngrams(captions, n=3):\n    ngram_counter = Counter()\n    \n    for caption in captions:\n        tokens = nltk.word_tokenize(caption)\n        for i in range(1, n + 1):  # Generate uni-, bi-, tri-grams\n            n_grams = ngrams(tokens, i)\n            ngram_counter.update(n_grams)\n    \n    return ngram_counter\n\n# Example usage:\ncaptions = [item['caption'] for item in trainval_data if item['caption']]\nngram_counter = extract_ngrams(captions, n=3)\n\nprint(f\"Top 10 N-Grams: {ngram_counter.most_common(10)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Compute Informative Weights for N-Grams","metadata":{}},{"cell_type":"code","source":"import math\n\ndef compute_pmi(ngram_counter, total_tokens):\n    pmi_scores = {}\n    ngram_totals = sum(ngram_counter.values())\n    \n    for ngram, count in ngram_counter.items():\n        p_ngram = count / ngram_totals\n        p_words = math.prod([ngram_counter[tuple([word])] / total_tokens for word in ngram])\n        if p_words > 0:  # Avoid division by zero\n            pmi_scores[ngram] = math.log(p_ngram / p_words, 2)\n    \n    return pmi_scores\n\n# Compute PMI\ntotal_tokens = sum(len(nltk.word_tokenize(caption)) for caption in captions)\npmi_scores = compute_pmi(ngram_counter, total_tokens)\n\n# Display top PMI scores\nsorted_pmi = sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)\nprint(f\"Top 10 PMI Scores: {sorted_pmi[:10]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Generate Weighted N-Gram Embeddings","metadata":{}},{"cell_type":"code","source":"!pip install -q sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Initialize SentenceTransformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with your preferred embedding model\n\ndef generate_weighted_embeddings_in_batches(ngram_pmi_scores, model, batch_size=32):\n    ngram_list = [' '.join(ngram) for ngram, _ in ngram_pmi_scores]\n    pmi_values = [pmi for _, pmi in ngram_pmi_scores]\n    \n    embeddings = {}\n    for i in range(0, len(ngram_list), batch_size):\n        batch_ngrams = ngram_list[i:i + batch_size]\n        batch_pmis = pmi_values[i:i + batch_size]\n        \n        # Generate embeddings for the batch\n        batch_embeddings = model.encode(batch_ngrams, show_progress_bar=True)\n        \n        # Scale embeddings with PMI values\n        for ngram, pmi, embedding in zip(batch_ngrams, batch_pmis, batch_embeddings):\n            weighted_embedding = embedding * pmi\n            embeddings[ngram] = weighted_embedding\n    \n    return embeddings\n\n# Generate weighted embeddings with batching\nweighted_embeddings = generate_weighted_embeddings_in_batches(sorted_pmi, model, batch_size=32)\n\n# Example: Check a weighted embedding\nexample_ngram = list(weighted_embeddings.keys())[1]\nprint(f\"N-Gram: {example_ngram}\")\nprint(f\"Weighted Embedding Shape: {weighted_embeddings[example_ngram].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ABOVE CODE IN DIFF FORM","metadata":{}},{"cell_type":"code","source":"\n\n# from sentence_transformers import SentenceTransformer\n# from joblib import Parallel, delayed\n# import numpy as np\n\n# # Initialize SentenceTransformer model\n# model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# def process_ngram(ngram, pmi, model):\n#     \"\"\"Generate a weighted embedding for a single n-gram.\"\"\"\n#     ngram_str = ' '.join(ngram)\n#     embedding = model.encode(ngram_str)\n#     return ngram_str, embedding * pmi\n\n# def generate_weighted_embeddings_parallel(ngram_pmi_scores, model, n_jobs=-1):\n#     \"\"\"Generate embeddings in parallel.\"\"\"\n#     results = Parallel(n_jobs=n_jobs)(\n#         delayed(process_ngram)(ngram, pmi, model) for ngram, pmi in ngram_pmi_scores\n#     )\n#     return dict(results)\n\n# # Generate weighted embeddings in parallel\n# weighted_embeddings = generate_weighted_embeddings_parallel(sorted_pmi[:1000], model, n_jobs=4)\n\n# # Example: Check a weighted embedding\n# example_ngram = list(weighted_embeddings.keys())[0]\n# print(f\"N-Gram: {example_ngram}\")\n# print(f\"Weighted Embedding Shape: {weighted_embeddings[example_ngram].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## We can save the computed embeddings as .npy or .json files for later use.","metadata":{}},{"cell_type":"code","source":"# np.save('weighted_ngram_embeddings.npy', weighted_embeddings)\n# print(\"Weighted embeddings saved as 'weighted_ngram_embeddings.npy'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# shutil.move('weighted_ngram_embeddings.npy', '/kaggle/working/weighted_ngram_embeddings.npy')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n\n# # Convert embeddings to a list (for JSON compatibility)\n# json_embeddings = {key: value.tolist() for key, value in weighted_embeddings.items()}\n\n# with open('weighted_ngram_embeddings.json', 'w') as f:\n#     json.dump(json_embeddings, f)\n# print(\"Weighted embeddings saved as 'weighted_ngram_embeddings.json'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# shutil.move('weighted_ngram_embeddings.json', '/kaggle/working/weighted_ngram_embeddings.json')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Code to Load Precomputed Weighted N-Gram Embeddings","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Path to the precomputed embeddings file\nn_gram_embeddings_path = \"/kaggle/input/weighted-n-gram-embeddings/weighted_ngram_embeddings.npy\"\n\n# Load the embeddings\ndef load_precomputed_embeddings(embeddings_path):\n    try:\n        embeddings = np.load(embeddings_path, allow_pickle=True).item()\n        print(f\"Loaded embeddings with {len(embeddings)} n-grams.\")\n        return embeddings\n    except Exception as e:\n        print(f\"Error loading embeddings: {e}\")\n        return None\n\n# Example usage\nweighted_embeddings = load_precomputed_embeddings(n_gram_embeddings_path)\n\n#my code\n# Inspect contents\nprint(f\"Number of N-Grams: {len(weighted_embeddings)}\")\n#my code ends here\n\n# Access an example n-gram and its embedding\nif weighted_embeddings:\n    example_ngram = list(weighted_embeddings.keys())[1]\n    print(f\"Example N-Gram: {example_ngram}\")\n    print(f\"Embedding Shape: {weighted_embeddings[example_ngram].shape}\")\n    print(f\"Embedding: {weighted_embeddings[example_ngram]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Viewing a .npy File","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n\n# # Load the .npy file\n# weighted_embeddings = np.load('weighted_ngram_embeddings.npy', allow_pickle=True).item()\n\n# # Inspect contents\n# print(f\"Number of N-Grams: {len(weighted_embeddings)}\")\n\n# # Example: View a single n-gram and its embedding\n# example_ngram = list(weighted_embeddings.keys())[0]\n# print(f\"N-Gram: {example_ngram}\")\n# print(f\"Embedding: {weighted_embeddings[example_ngram]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Viewing a .json File","metadata":{}},{"cell_type":"code","source":"# import json\n\n# # Load the .json file\n# with open('weighted_ngram_embeddings.json', 'r') as f:\n#     weighted_embeddings = json.load(f)\n\n# # Inspect contents\n# print(f\"Number of N-Grams: {len(weighted_embeddings)}\")\n\n# # Example: View a single n-gram and its embedding\n# example_ngram = list(weighted_embeddings.keys())[0]\n# print(f\"N-Gram: {example_ngram}\")\n# print(f\"Embedding: {weighted_embeddings[example_ngram]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization : Histogram of PMI Scores:","metadata":{},"attachments":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract PMI scores for visualization\npmi_scores = [np.linalg.norm(embedding) for embedding in weighted_embeddings.values()]\n\nplt.hist(pmi_scores, bins=30, color='blue', alpha=0.7)\nplt.title(\"Histogram of Weighted Embedding Norms\")\nplt.xlabel(\"Embedding Norm\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"here shape of Weighted N-Gram Embeddings is 384 and we want 700 so we will Adjusting Shape to Match Sent2Vec (700)","metadata":{}},{"cell_type":"markdown","source":"Using Linear Transformation we will change dimension from 384 to 700","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\n\n# Target dimension for Sent2Vec embeddings\ntarget_dim = 700\n\n# # Load precomputed embeddings\n# weighted_embeddings = np.load(\"weighted_n_gram_embeddings.npy\", allow_pickle=True).item()\n\n# Define a linear transformation\nlinear_proj = nn.Linear(384, target_dim)\n\n# Project embeddings\nprojected_embeddings = {\n    ngram: linear_proj(torch.tensor(embedding, dtype=torch.float32)).detach().numpy()\n    for ngram, embedding in weighted_embeddings.items()\n}\n\n# Save the transformed embeddings\nnp.save(\"sent2vec_projected_embeddings.npy\", projected_embeddings)\nprint(f\"Projected embeddings to shape {target_dim}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load and Inspect Projected Embeddings","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Path to the saved projected embeddings\nprojected_embeddings_path = \"sent2vec_projected_embeddings.npy\"\n\n# Load the embeddings\nprojected_embeddings = np.load(projected_embeddings_path, allow_pickle=True).item()\n\n\n# Count the total number of keys in the dictionary\ntotal_keys = len(projected_embeddings)\n\nprint(f\"Total number of keys in projected_embeddings: {total_keys}\")\n\n\n# Check the number of embeddings\nprint(f\"Number of embeddings loaded: {len(projected_embeddings)}\")\n\n# View the shape of the embeddings\nexample_ngram = list(projected_embeddings.keys())[0]  # Get an example n-gram\nexample_embedding = projected_embeddings[example_ngram]\n\nprint(f\"Example N-Gram: {example_ngram}\")\nprint(f\"Embedding Shape: {example_embedding.shape}\")\nprint(f\"First 5 values of Embedding: {example_embedding}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"saving above new shaped Weighted N-Gram Embeddings\n\n","metadata":{}},{"cell_type":"code","source":"\n# np.save('sent2vec_projected_embeddings.npy', projected_embeddings)\n# print(\"projected_embeddings saved as 'sent2vec_projected_embeddings.npy'.\")\n\n# import shutil\n# shutil.move('sent2vec_projected_embeddings.npy', '/kaggle/working/sent2vec_projected_embeddings.npy')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"now we will load new Weighted N-Gram Embeddings which is of shape 700 and print","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Path to the saved embeddings file\nprojected_embeddings_path = \"/kaggle/input/sent2vec-embeddings-700-dim/sent2vec_projected_embeddings.npy\"\n\n# Load the embeddings\nloaded_embeddings = np.load(projected_embeddings_path, allow_pickle=True).item()\n\n# Check the content\nprint(f\"Number of embeddings loaded: {len(loaded_embeddings)}\")\nexample_ngram = list(loaded_embeddings.keys())[0]\nprint(f\"Example N-Gram: {example_ngram}\")\nprint(f\"Embedding Shape: {loaded_embeddings[example_ngram].shape}\")\nprint(f\"First 5 values of the embedding: {loaded_embeddings[example_ngram][:5]}\")\nprint(f\"embedding: {loaded_embeddings[example_ngram]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"## K-Means clustering for generating proxy labels from sentence embeddings","metadata":{}},{"cell_type":"markdown","source":"1. Load Precomputed Embeddings","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Path to the saved embeddings\nembeddings_path = \"/kaggle/input/sent2vec-embeddings-700-dim/sent2vec_projected_embeddings.npy\"\n\n# Load the embeddings\nweighted_embeddings = np.load(embeddings_path, allow_pickle=True).item()\nprint(f\"Loaded {len(weighted_embeddings)} embeddings.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Prepare Embeddings for Clustering","metadata":{}},{"cell_type":"code","source":"# Prepare data for clustering\nembedding_list = list(weighted_embeddings.values())\nembedding_array = np.array(embedding_list)\nprint(f\"Shape of data for clustering: {embedding_array.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Apply K-Means Clustering","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Number of clusters\nnum_clusters = 10  # Adjust based on the dataset and requirements\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ncluster_labels = kmeans.fit_predict(embedding_array)\n\n# Save the cluster centroids and labels\ncentroids = kmeans.cluster_centers_\nnp.save(\"cluster_centroids.npy\", centroids)\nnp.save(\"cluster_labels.npy\", cluster_labels)\n\nprint(f\"Clustering complete with {num_clusters} clusters.\")\nprint(f\"Centroids shape: {centroids.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Assign Cluster Labels to N-Grams","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n\n# # Load the cluster labels and centroids\n# cluster_labels_path = \"cluster_labels.npy\"\n# centroids_path = \"cluster_centroids.npy\"\n\n# cluster_labels = np.load(cluster_labels_path)\n# centroids = np.load(centroids_path)\n\n# # Print cluster labels\n# print(\"Cluster Labels:\")\n# print(cluster_labels)  # Prints the cluster index assigned to each n-gram\n# print(f\"cluster label shape: {cluster_labels.shape}\")\n\n# # Print cluster centroids\n# print(\"\\nCluster Centroids:\")\n# print(f\"Centroids shape: {centroids.shape}\")\n# for i, centroid in enumerate(centroids):\n#     print(f\"Centroid {i}: {centroid}\")\n\n\n# Map n-grams to cluster labels\nngram_to_cluster = {ngram: label for ngram, label in zip(weighted_embeddings.keys(), cluster_labels)}\n\n# Save the mapping\nnp.save(\"ngram_to_cluster.npy\", ngram_to_cluster)\n\n# Example: Check a sample n-gram and its cluster\nexample_ngram = list(ngram_to_cluster.keys())[0]\nprint(f\"N-Gram: {example_ngram}, Cluster: {ngram_to_cluster[example_ngram]}\")\nprint(f\"Number of n-grams mapped: {len(ngram_to_cluster)}\")\n\n# example_ngram = list(ngram_to_cluster.keys())[0]\ncluster_label = ngram_to_cluster[example_ngram]\nprint(f\"Type of value: {type(cluster_label)}\")\nprint(f\"Value: {cluster_label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"saving centroids and cluster_labels","metadata":{}},{"cell_type":"code","source":"import shutil\nshutil.move('cluster_centroids.npy', '/kaggle/working/cluster_centroids.npy')\nshutil.move('cluster_labels.npy', '/kaggle/working/cluster_labels.npy')\nshutil.move('ngram_to_cluster.npy', '/kaggle/working/ngram_to_cluster.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"above proxy labels will be used in training VJSM-net","metadata":{}},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"## Semantic Space Construction","metadata":{}},{"cell_type":"markdown","source":"we will focus on:\n\n1. Using Weighted N-Gram Embeddings:\n\nYou have already computed the embeddings and applied clustering (K-Means) to group similar n-grams. We'll now use this information to construct a semantic space.\n\n2. Semantic Space Construction:\n\nCombine the cluster centroids with n-gram embeddings to represent a unified semantic space.","metadata":{}},{"cell_type":"markdown","source":"1. Load Necessary Data , Load the saved embeddings, cluster centroids, and mappings.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Paths to required files\nembeddings_path = \"/kaggle/input/sent2vec-embeddings-700-dim/sent2vec_projected_embeddings.npy\"\ncentroids_path = \"/kaggle/input/clusters-info/cluster_centroids.npy\"\ncluster_mapping_path = \"/kaggle/input/clusters-info/ngram_to_cluster.npy\"\n\n# Load data\nweighted_embeddings = np.load(embeddings_path, allow_pickle=True).item()\ncluster_centroids = np.load(centroids_path)\nngram_to_cluster = np.load(cluster_mapping_path, allow_pickle=True).item()\n\n# Print summaries\nprint(f\"Number of weighted embeddings: {len(weighted_embeddings)}\")\nprint(f\"Centroid Shape: {cluster_centroids.shape}\")\nprint(f\"Number of n-gram to cluster mappings: {len(ngram_to_cluster)}\")\n\nexample_ngram = list(weighted_embeddings.keys())[0]\nembedding_shape = np.array(weighted_embeddings[example_ngram]).shape\nprint(f\"Shape of an individual embedding: {embedding_shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Construct the Semantic Space:\n \nCombine the n-gram embeddings with their respective cluster centroids. This allows each embedding to be represented in a unified semantic space defined by the clusters.","metadata":{}},{"cell_type":"code","source":"def construct_semantic_space(embeddings, cluster_mapping, centroids):\n    semantic_space = {}\n    for ngram, embedding in embeddings.items():\n        cluster_label = cluster_mapping.get(ngram)\n        if cluster_label is not None:\n            centroid = centroids[cluster_label]\n            combined_representation = np.concatenate([embedding, centroid])\n            semantic_space[ngram] = combined_representation\n    return semantic_space\n\n# Construct the semantic space\nsemantic_space = construct_semantic_space(weighted_embeddings, ngram_to_cluster, cluster_centroids)\n\n# Example: Inspect a single entry in the semantic space\nexample_ngram = list(semantic_space.keys())[0]\nprint(f\"N-Gram: {example_ngram}\")\nprint(f\"Semantic Space Representation Shape: {semantic_space[example_ngram].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Print a subset (e.g., the first 10 elements) of the feature values for inspection\n# print(f\"First 10 values of the feature for Video ID {semantic_space}:\")\n# print(example_ngram[:5])  # Adjust the slice as needed\n# Print a few entries from the semantic_space\nprint(\"Sample entries from the semantic space:\\n\")\n\n# Limit the number of entries to print\nnum_samples = 5\nfor i, (ngram, combined_representation) in enumerate(semantic_space.items()):\n    if i >= num_samples:\n        break\n    print(f\"N-Gram: {ngram}\")\n    print(f\"Combined Representation (Shape: {combined_representation.shape}):\\n{combined_representation}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Save the Semantic Space of shape (1400,)","metadata":{}},{"cell_type":"code","source":"# Save the semantic space\nsemantic_space_path = \"semantic_space.npy\"\nnp.save(semantic_space_path, semantic_space)\nprint(f\"Semantic space saved to {semantic_space_path}.\")\n\nimport shutil\nshutil.move('semantic_space.npy', '/kaggle/working/semantic_space.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"note : No, we did not train a Sent2Vec model to generate caption embeddings. Instead, we used pre-existing SentenceTransformer embeddings to generate the Weighted N-Gram Embeddings and then processed these embeddings for clustering and semantic space construction.","metadata":{}},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"## Joint Modeling with VSJM-Net with input :\n\nKey Inputs:\n\nVisual features (e.g., ResNet-152 features of size 2048 per frame).\nSemantic embeddings from the constructed semantic space (e.g., size 1400).\n\nOutputs:\n\nJoint embeddings in a common semantic space.\nTrained model weights for downstream tasks (e.g., temporal modeling).","metadata":{}},{"cell_type":"markdown","source":"1. Prepare Data\nWe need:\n\nVisual Features: Pre-extracted ResNet features of size 2048.\nSemantic Features: Embeddings from the constructed semantic space of size 1400.\nProxy Labels: Cluster labels generated from K-Means clustering.","metadata":{}},{"cell_type":"markdown","source":"below first code loading train_features again ","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n\n# # Paths to required files\n# visual_features_path = \"/path/to/visual_features.npy\"  # Replace with actual path\n# semantic_space_path = \"semantic_space.npy\"\n# proxy_labels_path = \"cluster_labels.npy\"\n\n# # Load data\n# visual_features = np.load(visual_features_path, allow_pickle=True)\n# semantic_space = np.load(semantic_space_path, allow_pickle=True).item()\n# proxy_labels = np.load(proxy_labels_path)\n\n# # Prepare dataset\n# class JointFeatureDataset(Dataset):\n#     def __init__(self, visual_features, semantic_space, proxy_labels):\n#         self.visual_features = visual_features\n#         self.semantic_space = semantic_space\n#         self.proxy_labels = proxy_labels\n#         self.semantic_keys = list(semantic_space.keys())\n        \n#     def __len__(self):\n#         return len(self.semantic_keys)\n\n#     def __getitem__(self, idx):\n#         # Match visual and semantic features by index\n#         ngram = self.semantic_keys[idx]\n#         semantic_embedding = self.semantic_space[ngram]\n#         visual_embedding = self.visual_features[idx]  # Assuming same order\n#         label = self.proxy_labels[idx]\n        \n#         # Combine visual and semantic features\n#         combined_features = np.concatenate([visual_embedding, semantic_embedding])\n#         return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# # Initialize dataset and dataloader\n# dataset = JointFeatureDataset(visual_features, semantic_space, proxy_labels)\n# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# # Example: Inspect data\n# for features, labels in dataloader:\n#     print(f\"Feature Shape: {features.shape}, Label Shape: {labels.shape}\")\n#     break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"when train used from above loaded","metadata":{}},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset, DataLoader\n\n# # Path to semantic space and proxy labels\n# semantic_space_path = \"/kaggle/input/semantic-space/semantic_space.npy\"\n# proxy_labels_path = \"/kaggle/input/clusters-info/cluster_labels.npy\"\n\n# # Load the semantic space and proxy labels\n# semantic_space = np.load(semantic_space_path, allow_pickle=True).item()\n# proxy_labels = np.load(proxy_labels_path)\n\n# # Prepare dataset\n# class JointFeatureDataset(Dataset):\n#     def __init__(self, visual_features, semantic_space, proxy_labels):\n#         self.visual_features = visual_features  # Transformed train features\n#         self.semantic_space = semantic_space\n#         self.proxy_labels = proxy_labels\n#         self.semantic_keys = list(semantic_space.keys())  # Align with n-grams\n        \n#     def __len__(self):\n#         return len(self.semantic_keys)\n\n#     def __getitem__(self, idx):\n#         # Match visual and semantic features by index\n#         ngram = self.semantic_keys[idx]\n#         semantic_embedding = self.semantic_space[ngram]\n#         visual_embedding = self.visual_features[ngram]  # Assuming the n-gram aligns with video ID\n#         label = self.proxy_labels[idx]\n        \n#         # Combine visual and semantic features\n#         combined_features = np.concatenate([visual_embedding, semantic_embedding])\n#         return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# # Initialize dataset and dataloader\n# dataset = JointFeatureDataset(train_features, semantic_space, proxy_labels)\n# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# # Example: Inspect data\n# for features, labels in dataloader:\n#     print(f\"Feature Shape: {features.shape}, Label Shape: {labels.shape}\")\n#     break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"for data loading","metadata":{}},{"cell_type":"code","source":"# Path to semantic space and proxy labels\nsemantic_space_path = \"/kaggle/input/semantic-space/semantic_space.npy\"\nproxy_labels_path = \"/kaggle/input/clusters-info/cluster_labels.npy\"\n\n# Load the semantic space and proxy labels\nsemantic_space = np.load(semantic_space_path, allow_pickle=True).item()\nproxy_labels = np.load(proxy_labels_path)\n\n# Print the type and details of semantic_space\nprint(f\"Type of semantic_space: {type(semantic_space)}\")\n\n# Check if semantic_space is a dictionary\nif isinstance(semantic_space, dict):\n    print(f\"Number of entries in semantic_space: {len(semantic_space)}\")\n    \n    # Print a few keys and their corresponding shapes\n    for i, (key, value) in enumerate(semantic_space.items()):\n        print(f\"Key: {key}, Value shape: {np.array(value).shape}\")\n        if i >= 4:  # Limit to printing 5 entries\n            break\nelse:\n    print(\"semantic_space is not a dictionary. Checking shape...\")\n    print(f\"Shape of semantic_space: {semantic_space.shape}\")\n    print(\"Few entries of semantic_space:\")\n    print(semantic_space[:10])  # Assuming it's an array-like object\n\nimport numpy as np\n\n# Load the proxy_labels (Assuming already loaded)\n# proxy_labels = np.load(proxy_labels_path)\n\n# Print the shape of proxy_labels\nprint(f\"Shape of proxy_labels: {proxy_labels.shape}\")\n\n# Print a few entries of proxy_labels\nprint(\"Entries of proxy_labels:\")\nprint(proxy_labels[:10])  # Prints the first 10 entries\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to the annotations file\nannotations_path = \"/kaggle/input/annotations/youcookii_annotations_trainval/youcookii_annotations_trainval.json\"\n\n# Load annotations\nimport json\n\nwith open(annotations_path, 'r') as f:\n    annotations = json.load(f)\n\n# Example: Inspecting the structure\nprint(annotations.keys())  # Should contain 'database' or similar\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\n\n# Access the \"database\" key\nvideo_data = annotations[\"database\"]\n    \n# Mapping from n-grams to video IDs\nngram_to_video = {}\n\nfor video_id, video_info in video_data.items():\n    for annotation in video_info[\"annotations\"]:  # Access the list of annotations\n        caption = annotation[\"sentence\"]  # Extract the sentence for each segment\n        tokens = nltk.word_tokenize(caption)\n        for n in range(1, 4):  # Uni-, bi-, tri-grams\n            for ngram in ngrams(tokens, n):\n                ngram_to_video[ngram] = video_id\n\nprint(f\"Generated n-gram to video mapping for {len(ngram_to_video)} n-grams.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\n\n# Mapping from n-grams to video IDs\nngram_to_video = {}\n\n# Tokenize captions into n-grams and map to video IDs\nfor video_id, video_info in annotations[\"database\"].items():\n    for annotation in video_info[\"annotations\"]:\n        caption = annotation[\"sentence\"]\n        tokens = nltk.word_tokenize(caption)\n        for n in range(1, 4):  # Uni-, bi-, tri-grams\n            for ngram in ngrams(tokens, n):\n                ngram_to_video[ngram] = video_id\n\nprint(f\"Generated n-gram to video mapping for {len(ngram_to_video)} n-grams.\")\n\n\n# Print the first five key-value pairs\nfor i, (ngram, video_id) in enumerate(ngram_to_video.items()):\n    print(f\"N-gram: {ngram}, Video ID: {video_id}\")\n    if i == 4:  # Stop after printing five entries\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Inspect Keys in train_features, val_features, test_features\nPrint the keys in train_features:","metadata":{}},{"cell_type":"code","source":"print(f\"Keys in train_features: {list(train_features.keys())[:5]}\")\nprint(f\"Keys in test_features: {list(test_features.keys())[:5]}\")\nprint(f\"Keys in val_features: {list(val_features.keys())[:5]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Inspect Video IDs in ngram_to_video\nPrint a sample of the video IDs being mapped in ngram_to_video:","metadata":{}},{"cell_type":"code","source":"video_ids_in_ngram_to_video = set(ngram_to_video.values())\nprint(f\"Video IDs in ngram_to_video: {list(video_ids_in_ngram_to_video)[:10]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compare Keys\nCheck if there is any overlap between the keys in train_features and ngram_to_video:","metadata":{}},{"cell_type":"code","source":"overlap = set(train_features.keys()) & video_ids_in_ngram_to_video\nprint(f\"Number of overlapping train video IDs: {len(overlap)}\")\n\n\noverlap_val = set(val_features.keys()) & video_ids_in_ngram_to_video\nprint(f\"Number of overlapping validity video IDs: {len(overlap_val)}\")\n\n\noverlap_test = set(test_features.keys()) & video_ids_in_ngram_to_video\nprint(f\"Number of overlapping test video IDs: {len(overlap_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Normalize train_features Keys: Extract the actual video IDs from the complex paths in train_features keys. For example:\n\n'224/YUWG15N2yw4/0010' â†’ 'YUWG15N2yw4'\nCode for Normalization: Use the following code to normalize train_features keys:","metadata":{}},{"cell_type":"markdown","source":"run below only one time but we are normalizing it at loading time at starting only","metadata":{}},{"cell_type":"code","source":"# Normalize train_features keys to extract video IDs\n# run bvelow only one time\n# normalized_train_features = {\n#     key.split('/')[1]: features for key, features in train_features.items()\n# }\n\n# # Reassign normalized train_features\n# train_features = normalized_train_features\n\n# # Debug normalized keys\n# print(f\"Normalized train_features keys: {list(train_features.keys())[:5]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check overlap after normalization\noverlap = set(train_features.keys()) & video_ids_in_ngram_to_video\nprint(f\"Number of overlapping video IDs after normalization: {len(overlap)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Filter the Semantic Space: Now, filter the semantic_space using only n-grams that map to overlapping video IDs.","metadata":{}},{"cell_type":"code","source":"print(f\"Sample keys in semantic_space: {list(semantic_space.keys())[:5]}\")\nprint(f\"Sample keys in ngram_to_video: {list(ngram_to_video.keys())[:5]}\")\noverlap_ngrams = set(semantic_space.keys()) & set(ngram_to_video.keys())\nprint(f\"Number of overlapping n-grams: {len(overlap_ngrams)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert ngram_to_video keys from tuples to space-separated strings\nngram_to_video_standardized = {\n    ' '.join(ngram): video_id for ngram, video_id in ngram_to_video.items()\n}\n\n# Debug standardized keys\nprint(f\"Sample keys in ngram_to_video_standardized: {list(ngram_to_video_standardized.keys())[:5]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate overlap after standardizing ngram_to_video keys\noverlap_ngrams = set(semantic_space.keys()) & set(ngram_to_video_standardized.keys())\nprint(f\"Number of overlapping n-grams after standardization: {len(overlap_ngrams)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Filter semantic space based on overlapping video IDs\n# filtered_semantic_space = {\n#     ngram: embedding\n#     for ngram, embedding in semantic_space.items()\n#     if ngram in ngram_to_video and ngram_to_video[ngram] in overlap\n# }\n\n# print(f\"Filtered semantic space size: {len(filtered_semantic_space)}\")\n\nfiltered_semantic_space = {\n    ngram: embedding\n    for ngram, embedding in semantic_space.items()\n    if ngram in ngram_to_video_standardized and ngram_to_video_standardized[ngram] in overlap\n}\nprint(f\"Filtered semantic space size: {len(filtered_semantic_space)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Align Proxy Labels: Ensure the proxy labels correspond to the filtered semantic_space","metadata":{}},{"cell_type":"code","source":"# Align proxy labels with the filtered semantic space\naligned_proxy_labels = [\n    proxy_labels[idx] for idx, ngram in enumerate(semantic_space.keys())\n    if ngram in filtered_semantic_space\n]\n\nprint(f\"Aligned proxy labels size: {len(aligned_proxy_labels)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect keys in semantic_space\nprint(f\"Sample semantic_space keys: {list(semantic_space.keys())[:5]}\")\n\n# Inspect keys in ngram_to_video\nprint(f\"Sample ngram_to_video keys: {list(ngram_to_video.keys())[:5]}\")\n\n#new one\nprint(f\"Sample ngram_to_video keys: {list(ngram_to_video_standardized.keys())[:5]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check overlap between semantic_space keys and ngram_to_video keys\n# overlap_ngrams = set(semantic_space.keys()) & set(ngram_to_video.keys())\n# print(f\"Number of overlapping n-grams: {len(overlap_ngrams)}\")\n# print(f\"Sample overlapping n-grams: {list(overlap_ngrams)[:5]}\")\n\n\n# Check overlap between semantic_space keys and ngram_to_video keys\noverlap_ngrams = set(semantic_space.keys()) & set(ngram_to_video_standardized.keys())\nprint(f\"Number of overlapping n-grams: {len(overlap_ngrams)}\")\nprint(f\"Sample overlapping n-grams: {list(overlap_ngrams)[:5]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Normalize and Align Keys\nIf there are tokenization or formatting differences (e.g., case sensitivity, extra whitespace), normalize the keys in both semantic_space and ngram_to_video. For example:","metadata":{}},{"cell_type":"code","source":"# # Normalize keys by converting to lowercase and stripping whitespace\n# normalized_semantic_space = {\n#     \" \".join(ngram).lower().strip(): embedding for ngram, embedding in semantic_space.items()\n# }\n# normalized_ngram_to_video = {\n#     \" \".join(ngram).lower().strip(): video_id for ngram, video_id in ngram_to_video.items()\n# }\n\n# # Check overlap after normalization\n# overlap_ngrams = set(normalized_semantic_space.keys()) & set(normalized_ngram_to_video.keys())\n# print(f\"Number of overlapping n-grams after normalization: {len(overlap_ngrams)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Filter semantic space using normalized n-grams\n# filtered_semantic_space = {\n#     ngram: embedding\n#     for ngram, embedding in normalized_semantic_space.items()\n#     if ngram in normalized_ngram_to_video and normalized_ngram_to_video[ngram] in overlap\n# }\n\n# print(f\"Filtered semantic space size after normalization: {len(filtered_semantic_space)}\")\n\n# # Align proxy labels\n# aligned_proxy_labels = [\n#     proxy_labels[idx] for idx, ngram in enumerate(semantic_space.keys())\n#     if ngram in filtered_semantic_space\n# ]\n\n# print(f\"Aligned proxy labels size after normalization: {len(aligned_proxy_labels)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Filter Semantic Space and Align Proxy Labels\nOnce the n-grams are aligned, retry the filtering:","metadata":{}},{"cell_type":"markdown","source":"### MAIN VJSM , Dataset Preparation Code for VJSM","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Purpose of the Code\nThe goal of this code is to:\n\nPrepare the dataset for training.\nCombine visual and semantic features into a unified representation for each n-gram.\nProvide the combined features and labels for training a model (e.g., VSJM-Net).\nThe dataset structure is essential for supervised learning, where both input features and target labels are required.","metadata":{}},{"cell_type":"code","source":"class JointFeatureDataset(Dataset):\n    def __init__(self, visual_features, semantic_space, proxy_labels, ngram_to_video):\n        self.visual_features = visual_features  # Normalized train or validity features\n        self.semantic_space = semantic_space  # Filtered semantic space\n        self.proxy_labels = proxy_labels  # Aligned proxy labels\n        self.ngram_to_video = ngram_to_video  # N-gram to video mapping\n        self.semantic_keys = list(semantic_space.keys())  # Keys of semantic space\n        \n    def __len__(self):\n        return len(self.semantic_keys)\n\n    def __getitem__(self, idx):\n        ngram = self.semantic_keys[idx]\n        semantic_embedding = self.semantic_space[ngram]\n        video_id = self.ngram_to_video[ngram]\n        visual_embedding = self.visual_features[video_id]\n        \n        # Debugging the dimensions\n        print(f\"Visual embedding shape: {visual_embedding.shape}\")\n        print(f\"Semantic embedding shape: {semantic_embedding.shape}\")\n        \n        # Apply mean pooling if needed\n        if len(visual_embedding.shape) > 1:  # Check if 2D\n            visual_embedding = np.mean(visual_embedding, axis=0)\n        \n        label = self.proxy_labels[idx]\n        \n        # Combine visual and semantic features\n        combined_features = np.concatenate([visual_embedding, semantic_embedding])\n        return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# Test the dataset to inspect dimensions\ndataset = JointFeatureDataset(\n    visual_features=train_features,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\n# Fetch the first item to inspect the dimensions\nfeatures, label = dataset[0]\n# print(f\"Sample Features Shape: {features.shape}\")\n# print(f\"Sample Label: {label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output indicates that:\n\nVisual embedding: (500, 2048) â†’ Frame-level features (500 frames, each of size 2048).\nSemantic embedding: (1400,) â†’ A single 1D embedding.\nThese dimensions are incompatible for concatenation, so adjustments are needed. Hereâ€™s how you can proceed:","metadata":{}},{"cell_type":"markdown","source":"Final Dataset Code:\n\nExpected Outputs\nCombined Features Shape: (batch_size, 2048 + 2048) if both embeddings are reduced to 2048.\nLabel Shape: (batch_size,)","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class JointFeatureDataset(Dataset):\n    def __init__(self, visual_features, semantic_space, proxy_labels, ngram_to_video):\n        self.visual_features = visual_features  # Normalized train features\n        self.semantic_space = semantic_space  # Filtered semantic space\n        self.proxy_labels = proxy_labels  # Aligned proxy labels\n        self.ngram_to_video = ngram_to_video  # N-gram to video mapping\n        self.semantic_keys = list(semantic_space.keys())  # Keys of semantic space\n        \n    def __len__(self):\n        return len(self.semantic_keys)\n\n    def __getitem__(self, idx):\n        ngram = self.semantic_keys[idx]\n        semantic_embedding = self.semantic_space[ngram]\n        video_id = self.ngram_to_video[ngram]\n        visual_embedding = self.visual_features[video_id]\n        \n        # Reduce visual embedding to 1D using mean pooling\n        visual_embedding = np.mean(visual_embedding, axis=0)\n        \n        # Reduce semantic embedding to match visual embedding size (optional slicing)\n        semantic_embedding = semantic_embedding[:2048]\n        \n        label = self.proxy_labels[idx]\n        \n        # Combine visual and semantic features\n        combined_features = np.concatenate([visual_embedding, semantic_embedding])\n        return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# Test the dataset\ndataset = JointFeatureDataset(\n    visual_features=train_features,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\n# Test a batch of data\nfor features, labels in DataLoader(dataset, batch_size=32, shuffle=True):\n    print(f\"Feature batch shape: {features.shape}\")\n    print(f\"Label batch shape: {labels.shape}\")\n    break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VSJMNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(VSJMNet, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, hidden_dim)  # Penultimate layer\n        self.fc5 = nn.Linear(hidden_dim, output_dim)  # Final layer\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x, return_penultimate=False):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.dropout(x)\n        penultimate_output = self.relu(self.fc4(x))\n        x = self.fc5(penultimate_output)\n        \n        if return_penultimate:\n            return penultimate_output\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\ninput_dim = 1400 + 2048  # Combined visual and semantic feature size\nhidden_dim = 1024  # Hidden layer dimension\noutput_dim = 100  # Number of proxy labels (num_clusters)\n\n# Initialize the model\nmodel = VSJMNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n\n# Optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()  # Proxy labels are categorical\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Example input\n# x = torch.rand(11, 3448)  # Batch of 32 samples\n# output = model(x)\n\n# print(f\"Output shape: {output.shape}\")  # Should be (32, output_dim)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class JointFeatureDataset(Dataset):\n    def __init__(self, visual_features, semantic_space, proxy_labels, ngram_to_video):\n        self.visual_features = visual_features  # Normalized train features\n        self.semantic_space = semantic_space  # Filtered semantic space\n        self.proxy_labels = proxy_labels  # Aligned proxy labels\n        self.ngram_to_video = ngram_to_video  # N-gram to video mapping\n        self.semantic_keys = list(semantic_space.keys())  # Keys of semantic space\n        \n    def __len__(self):\n        return len(self.semantic_keys)\n\n    def __getitem__(self, idx):\n        ngram = self.semantic_keys[idx]\n        semantic_embedding = self.semantic_space[ngram]\n        video_id = self.ngram_to_video[ngram]\n        visual_embedding = self.visual_features[video_id]\n        \n        # Apply mean pooling to reduce visual_embedding to 1D\n        if len(visual_embedding.shape) > 1:  # Check if 2D (e.g., [500, 2048])\n            visual_embedding = np.mean(visual_embedding, axis=0)\n        \n        label = self.proxy_labels[idx]\n        \n        # Combine visual and semantic features\n        combined_features = np.concatenate([visual_embedding, semantic_embedding])\n        return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the dataset\ndataset = JointFeatureDataset(\n    visual_features=train_features,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\n# Test a single sample\nfeatures, label = dataset[0]\nprint(f\"Combined features shape: {features.shape}\")  # Should be (2048 + 1400,)\n\n# Test a batch of samples\nfor features, labels in DataLoader(dataset, batch_size=32, shuffle=True):\n    print(f\"Feature batch shape: {features.shape}\")\n    print(f\"Label batch shape: {labels.shape}\")\n    break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def __getitem__(self, idx):\n    ngram = self.semantic_keys[idx]\n    semantic_embedding = self.semantic_space[ngram]\n    video_id = self.ngram_to_video[ngram]\n    visual_embedding = self.visual_features[video_id]\n    \n    # Debug: Print shapes before processing\n    print(f\"Raw visual embedding shape: {visual_embedding.shape}\")\n    print(f\"Raw semantic embedding shape: {semantic_embedding.shape}\")\n    \n    # Apply mean pooling to reduce visual embedding to 1D if it's 2D\n    if len(visual_embedding.shape) > 1:  # If 2D (e.g., [500, 2048])\n        visual_embedding = np.mean(visual_embedding, axis=0)\n    \n    # Debug: Print shapes after mean pooling\n    print(f\"Processed visual embedding shape: {visual_embedding.shape}\")\n    \n    # Combine visual and semantic features\n    combined_features = np.concatenate([visual_embedding, semantic_embedding])\n    label = self.proxy_labels[idx]\n    \n    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\ndef __getitem__(self, idx):\n    ngram = self.semantic_keys[idx]\n    semantic_embedding = self.semantic_space[ngram]\n    video_id = self.ngram_to_video[ngram]\n    visual_embedding = self.visual_features[video_id]\n    \n    # Apply mean pooling to reduce visual embedding to 1D\n    visual_embedding = np.mean(visual_embedding, axis=0) if len(visual_embedding.shape) > 1 else visual_embedding\n    \n    # Combine visual and semantic features\n    combined_features = np.concatenate([visual_embedding, semantic_embedding])\n    label = self.proxy_labels[idx]\n    \n    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the dataset\ndataset = JointFeatureDataset(\n    visual_features=train_features,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\n# Test a single sample\nfeatures, label = dataset[0]\nprint(f\"Sample combined features shape: {features.shape}\")  # Should be (2048 + 1400,)\n\n# Test a batch of samples\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\nfor features, labels in dataloader:\n    print(f\"Feature batch shape: {features.shape}\")  # Should be (batch_size, 2048 + 1400)\n    print(f\"Label batch shape: {labels.shape}\")      # Should be (batch_size,)\n    break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Training loop\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for features, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        # Move data to GPU if available\n        features = features.to(\"cuda\") if torch.cuda.is_available() else features\n        labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n        model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## @@ For both val and train split code saving final embedding for next step code starts here @@@@@@@@@","metadata":{}},{"cell_type":"code","source":"class JointFeatureDataset(Dataset):\n    def __init__(self, visual_features, semantic_space, proxy_labels, ngram_to_video):\n        self.visual_features = visual_features  # Normalized train features\n        self.semantic_space = semantic_space  # Filtered semantic space\n        self.proxy_labels = proxy_labels  # Aligned proxy labels\n        self.ngram_to_video = ngram_to_video  # N-gram to video mapping\n\n        # Filter out invalid n-grams\n        self.semantic_keys = [\n            ngram for ngram in semantic_space.keys()\n            if ngram in ngram_to_video and ngram_to_video[ngram] in visual_features\n        ]\n\n        if not self.semantic_keys:\n            raise ValueError(\"No valid semantic keys found. Please check your data alignment.\")\n\n    def __len__(self):\n        return len(self.semantic_keys)\n\n    def __getitem__(self, idx):\n        ngram = self.semantic_keys[idx]\n        semantic_embedding = self.semantic_space[ngram]\n        video_id = self.ngram_to_video[ngram]\n        visual_embedding = self.visual_features[video_id]\n\n        # Apply mean pooling to reduce visual_embedding to 1D\n        if len(visual_embedding.shape) > 1:  # Check if 2D (e.g., [500, 2048])\n            visual_embedding = np.mean(visual_embedding, axis=0)\n\n        label = self.proxy_labels[idx]\n\n        # Combine visual and semantic features\n        combined_features = np.concatenate([visual_embedding, semantic_embedding])\n        return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\n\n# Split train_features into training and validation sets (70-30 split)\ntrain_keys, val_keys = train_test_split(list(train_features.keys()), test_size=0.3, random_state=42)\n\n# Create train and validation feature dictionaries\ntrain_features_split = {key: train_features[key] for key in train_keys}\nval_features_split = {key: train_features[key] for key in val_keys}\n\n# Create datasets for training and validation\ndataset_train = JointFeatureDataset(\n    visual_features=train_features,\n    # visual_features=train_features_split,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\ndataset_val = JointFeatureDataset(\n    visual_features=val_features_split,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n    \n)\n\n# Create dataloaders for training and validation\ntrain_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset_val, batch_size=32, shuffle=False)\n\n# Training loop\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    # Training Phase\n    running_train_loss = 0.0\n    model.train()\n    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        # Move data to GPU if available\n        features = features.to(\"cuda\") if torch.cuda.is_available() else features\n        labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n        model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item()\n    \n    # Validation Phase\n    running_val_loss = 0.0\n    model.eval()\n    with torch.no_grad():\n        for features, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            # Move data to GPU if available\n            features = features.to(\"cuda\") if torch.cuda.is_available() else features\n            labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n\n            # Forward pass\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n\n            running_val_loss += loss.item()\n\n    # Calculate average losses for the epoch\n    avg_train_loss = running_train_loss / len(train_loader)\n    avg_val_loss = running_val_loss / len(val_loader)\n\n    # Print the losses for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving After Training\ntorch.save(model.state_dict(), \"VSJM_model.pth\")\nprint(\"Final VSJM model saved.\")\n\n\n#saving both model and optimizer:\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n}, \"checkpoint.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.move('VSJM_model.pth', '/kaggle/working/VSJM_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Loading the Model Later\nmodel.load_state_dict(torch.load(\"VSJM_model.pth\"))\nmodel.eval()  # Set model to evaluation mode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### saving joint embeddings for using ahead","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Split train_features into training and validation sets (80-20 split)\ntrain_keys, val_keys = train_test_split(list(train_features.keys()), test_size=0.2, random_state=42)\n\n# Create train and validation feature dictionaries\ntrain_features_split = {key: train_features[key] for key in train_keys}\nval_features_split = {key: train_features[key] for key in val_keys}\n\n\n# Create datasets for training and validation\ndataset_train = JointFeatureDataset(\n    visual_features=train_features_split,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\ndataset_val = JointFeatureDataset(\n    visual_features=val_features_split,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\n\n\n\n# Create dataloaders for training and validation\ntrain_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset_val, batch_size=32, shuffle=False)\n\n\n\n# Hyperparameters\ninput_dim = 3448  # Combined feature size\nhidden_dim = 512  # Hidden layer size\noutput_dim = 100  # Number of proxy labels (clusters)\nnum_epochs = 1\nbatch_size = 32\n\n# Initialize model, optimizer, and loss function\nmodel = VSJMNet(input_dim, hidden_dim, output_dim)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    running_train_loss = 0.0\n    # Training Phase\n    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        features = features.to(\"cuda\") if torch.cuda.is_available() else features\n        labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n        model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item()\n    \n    # Validation Phase\n    running_val_loss = 0.0\n    model.eval()\n    with torch.no_grad():\n        for features, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            features = features.to(\"cuda\") if torch.cuda.is_available() else features\n            labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n\n            # Forward pass\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n\n            running_val_loss += loss.item()\n\n    # Calculate average losses for the epoch\n    avg_train_loss = running_train_loss / len(train_loader)\n    avg_val_loss = running_val_loss / len(val_loader)\n\n    # Print the losses for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n\n\n# Hyperparameters\ninput_dim = 3448  # Combined feature size\nhidden_dim = 512  # Hidden layer size\noutput_dim = 100  # Number of proxy labels (clusters)\nnum_epochs = 1\nbatch_size = 32\n\n# Initialize model, optimizer, and loss function\nmodel = VSJMNet(input_dim, hidden_dim, output_dim)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    running_train_loss = 0.0\n    # Training Phase\n    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        features = features.to(\"cuda\") if torch.cuda.is_available() else features\n        labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n        model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item()\n    \n    # Validation Phase\n    running_val_loss = 0.0\n    model.eval()\n    with torch.no_grad():\n        for features, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            features = features.to(\"cuda\") if torch.cuda.is_available() else features\n            labels = labels.to(\"cuda\") if torch.cuda.is_available() else labels\n\n            # Forward pass\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n\n            running_val_loss += loss.item()\n\n    # Calculate average losses for the epoch\n    avg_train_loss = running_train_loss / len(train_loader)\n    avg_val_loss = running_val_loss / len(val_loader)\n\n    # Print the losses for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract embeddings for train and validation datasets\nmodel.eval()\npenultimate_activations_train = []\npenultimate_activations_val = []\n\n# Extract embeddings for the training set\nwith torch.no_grad():\n    for features, _ in train_loader:\n        features = features.to(\"cuda\")\n        embeddings = model(features, return_penultimate=True)\n        penultimate_activations_train.append(embeddings.cpu().numpy())\n\n# Extract embeddings for the validation set\nwith torch.no_grad():\n    for features, _ in val_loader:\n        features = features.to(\"cuda\")\n        embeddings = model(features, return_penultimate=True)\n        penultimate_activations_val.append(embeddings.cpu().numpy())\n\n# Save the activations\npenultimate_activations_train = np.vstack(penultimate_activations_train)\npenultimate_activations_val = np.vstack(penultimate_activations_val)\n\n# Save embeddings to files\nnp.save(\"vsjm_joint_embeddings_train.npy\", penultimate_activations_train)\nnp.save(\"vsjm_joint_embeddings_val.npy\", penultimate_activations_val)\n\nprint(f\"Train Embeddings shape: {penultimate_activations_train.shape}\")\nprint(f\"Validation Embeddings shape: {penultimate_activations_val.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved embeddings\nvsjm_joint_embeddings_train = np.load(\"vsjm_joint_embeddings_train.npy\")\nvsjm_joint_embeddings_val = np.load(\"vsjm_joint_embeddings_val.npy\")\n\n# Print the shapes\nprint(f\"Train Embeddings shape: {vsjm_joint_embeddings_train.shape}\")\nprint(f\"Validation Embeddings shape: {vsjm_joint_embeddings_val.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#saving these embeddings\nimport shutil\nshutil.move('vsjm_joint_embeddings_train.npy', '/kaggle/working/vsjm_joint_embeddings_train.npy')\nshutil.move('vsjm_joint_embeddings_val.npy', '/kaggle/working/vsjm_joint_embeddings_val.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"saving as dictionary","metadata":{}},{"cell_type":"code","source":"# # Initialize dictionaries to store embeddings\n# vsjm_joint_embeddings_train_dict = {}\n# vsjm_joint_embeddings_val_dict = {}\n\n# # Extract embeddings for the training set\n# model.eval()\n# with torch.no_grad():\n#     for i, (features, labels) in enumerate(train_loader):\n#         # Move features to device\n#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#         features = features.to(device)\n        \n#         # Forward pass to extract penultimate layer embeddings\n#         embeddings = model(features, return_penultimate=True).cpu().numpy()\n        \n#         # Store embeddings in the dictionary with video IDs as keys\n#         batch_video_ids = [list(train_features_split.keys())[idx] for idx in range(i * batch_size, i * batch_size + len(features))]\n#         for video_id, embedding in zip(batch_video_ids, embeddings):\n#             vsjm_joint_embeddings_train_dict[video_id] = embedding\n\n# # Save the training embeddings dictionary as a .npy file\n# np.save(\"vsjm_joint_embeddings_train_dict.npy\", vsjm_joint_embeddings_train_dict)\n# print(\"Saved training embeddings as a dictionary with video IDs as keys.\")\n\n# # Extract embeddings for the validation set\n# with torch.no_grad():\n#     for i, (features, labels) in enumerate(val_loader):\n#         # Move features to device\n#         features = features.to(device)\n        \n#         # Forward pass to extract penultimate layer embeddings\n#         embeddings = model(features, return_penultimate=True).cpu().numpy()\n        \n#         # Store embeddings in the dictionary with video IDs as keys\n#         batch_video_ids = [list(val_features_split.keys())[idx] for idx in range(i * batch_size, i * batch_size + len(features))]\n#         for video_id, embedding in zip(batch_video_ids, embeddings):\n#             vsjm_joint_embeddings_val_dict[video_id] = embedding\n\n# # Save the validation embeddings dictionary as a .npy file\n# np.save(\"vsjm_joint_embeddings_val_dict.npy\", vsjm_joint_embeddings_val_dict)\n# print(\"Saved validation embeddings as a dictionary with video IDs as keys.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# Initialize dictionaries to store embeddings\nvsjm_joint_embeddings_train_dict = {}\nvsjm_joint_embeddings_val_dict = {}\n\n# Determine the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\nbatch_size = train_loader.batch_size  # Assuming batch_size is defined in your DataLoader\n\n# Extract embeddings for the training set\nmodel.eval()\nwith torch.no_grad():\n    print(\"Extracting embeddings for training set...\")\n    for i, (features, labels) in enumerate(train_loader):\n        # Move features to the device\n        features = features.to(device)\n        \n        # Forward pass to extract penultimate layer embeddings\n        embeddings = model(features, return_penultimate=True).cpu().numpy()\n        \n        # # Ensure batch_video_ids do not exceed the number of available keys\n        max_index = len(train_features_split.keys())\n        batch_video_ids = [\n            list(train_features_split.keys())[idx]\n            for idx in range(i * batch_size, min(i * batch_size + len(features), max_index))\n        ]\n\n        # batch_video_ids = [\n        #     list(ngram_to_video_standardized.values())[idx]\n        #     for idx in range(i * batch_size, i * batch_size + len(features))\n        # ]\n\n\n\n        \n        \n        for video_id, embedding in zip(batch_video_ids, embeddings):\n            vsjm_joint_embeddings_train_dict[video_id] = embedding\n\n# Save the training embeddings dictionary as a .npy file\nnp.save(\"vsjm_joint_embeddings_train_dict.npy\", vsjm_joint_embeddings_train_dict)\nprint(\"Saved training embeddings as a dictionary with video IDs as keys.\")\n\n# Extract embeddings for the validation set\nwith torch.no_grad():\n    print(\"Extracting embeddings for validation set...\")\n    for i, (features, labels) in enumerate(val_loader):\n        # Move features to the device\n        features = features.to(device)\n        \n        # Forward pass to extract penultimate layer embeddings\n        embeddings = model(features, return_penultimate=True).cpu().numpy()\n        \n        # Ensure batch_video_ids do not exceed the number of available keys\n        max_index = len(val_features_split.keys())\n        batch_video_ids = [\n            list(val_features_split.keys())[idx]\n            for idx in range(i * batch_size, min(i * batch_size + len(features), max_index))\n        ]\n\n\n        # #or use this\n        # batch_video_ids = [\n        #     list(ngram_to_video_standardized.values())[idx]\n        #     for idx in range(i * batch_size, i * batch_size + len(features))\n        # ]\n\n\n\n        \n        \n        for video_id, embedding in zip(batch_video_ids, embeddings):\n            vsjm_joint_embeddings_val_dict[video_id] = embedding\n\n# Save the validation embeddings dictionary as a .npy file\nnp.save(\"vsjm_joint_embeddings_val_dict.npy\", vsjm_joint_embeddings_val_dict)\nprint(\"Saved validation embeddings as a dictionary with video IDs as keys.\")\n\n# Debugging: Print sizes of the generated dictionaries\nprint(f\"Number of training embeddings: {len(vsjm_joint_embeddings_train_dict)}\")\nprint(f\"Number of validation embeddings: {len(vsjm_joint_embeddings_val_dict)}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Move files to the desired location (if required, e.g., Kaggle workspace)\nimport shutil\nshutil.move('vsjm_joint_embeddings_train_dict.npy', '/kaggle/working/vsjm_joint_embeddings_train_dict.npy')\nshutil.move('vsjm_joint_embeddings_val_dict.npy', '/kaggle/working/vsjm_joint_embeddings_val_dict.npy')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved embeddings\nvsjm_joint_embeddings_train = np.load(\"vsjm_joint_embeddings_train_dict.npy\", allow_pickle=True).item()\nvsjm_joint_embeddings_val = np.load(\"vsjm_joint_embeddings_val_dict.npy\", allow_pickle=True).item()\n\nprint(f\"Loaded {len(vsjm_joint_embeddings_train)} train video embeddings.\")\nprint(f\"Loaded {len(vsjm_joint_embeddings_val)} validation video embeddings.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the saved embeddings dictionaries (Samples)\nprint(\"VSJM Joint Embeddings Train Dictionary (Sample):\")\nfor video_id, embedding in list(vsjm_joint_embeddings_train.items())[:5]:\n    print(f\"Video ID: {video_id}, Embedding Shape: {embedding.shape}\")\n\nprint(\"VSJM Joint Embeddings Validation Dictionary (Sample):\")\nfor video_id, embedding in list(vsjm_joint_embeddings_val.items())[:5]:\n    print(f\"Video ID: {video_id}, Embedding Shape: {embedding.shape}\")\n\n\n# Print sample embeddings from the training dictionary\nprint(\"VSJM Joint Embeddings Train Dictionary (Sample):\")\nfor video_id, embedding in list(vsjm_joint_embeddings_train.items())[:1]:\n    print(f\"Video ID: {video_id}\")\n    print(f\"Embedding Shape: {embedding.shape}\")\n    print(f\"Embedding Values: {embedding}\\n\")\n\n# # Print sample embeddings from the validation dictionary\n# print(\"VSJM Joint Embeddings Validation Dictionary (Sample):\")\n# for video_id, embedding in list(vsjm_joint_embeddings_val.items())[:1]:\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"Embedding Shape: {embedding.shape}\")\n#     print(f\"Embedding Values: {embedding}\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Optionally print the full dictionaries\n# print(\"VSJM Joint Embeddings Train Dictionary (Full):\")\n# for video_id, embedding in vsjm_joint_embeddings_train.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"Embedding: {embedding}\\n\")\n\n# print(\"VSJM Joint Embeddings Validation Dictionary (Full):\")\n# for video_id, embedding in vsjm_joint_embeddings_val.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"Embedding: {embedding}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## @@ For val train code ends here @@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"#### Extract Penultimate Layer Embeddings\nModify the forward pass of VSJMNet to return the activations of the penultimate layer (fc4), which serve as the joint embeddings.","metadata":{}},{"cell_type":"code","source":"class VSJMNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(VSJMNet, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, hidden_dim)  # Penultimate layer\n        self.fc5 = nn.Linear(hidden_dim, output_dim)  # Final layer\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x, return_penultimate=False):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.dropout(x)\n        penultimate_output = self.relu(self.fc4(x))\n        x = self.fc5(penultimate_output)\n        \n        if return_penultimate:\n            return penultimate_output\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ##### Save Penultimate Embeddings","metadata":{}},{"cell_type":"code","source":"dataset = JointFeatureDataset(\n    visual_features=train_features,\n    semantic_space=filtered_semantic_space,\n    proxy_labels=aligned_proxy_labels,\n    # ngram_to_video=normalized_ngram_to_video,\n    ngram_to_video=ngram_to_video_standardized,\n)\nfeatures, label = dataset[0]\nprint(f\"Combined features shape: {features.shape}\")\nprint(f\"Label: {label}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### printing this combined feature","metadata":{}},{"cell_type":"code","source":"# Fetch a specific data point from the dataset\nfeatures, label = dataset[1]\n\n# Print the combined features and label\nprint(f\"Combined features shape: {features.shape}\")\nprint(f\"Combined features: {features}\")  # Print the entire feature vector\nprint(f\"Label: {label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"upto above VJSM is done and working fine ","metadata":{}},{"cell_type":"markdown","source":"## BELOW WILL CREATE vsjm_joint_embeddings FOR NEXT STEP\n","metadata":{}},{"cell_type":"code","source":"class VSJMNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(VSJMNet, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, hidden_dim)  # Penultimate layer\n        self.fc5 = nn.Linear(hidden_dim, output_dim)  # Final layer\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x, return_penultimate=False):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.dropout(x)\n        penultimate_output = self.relu(self.fc4(x))\n        x = self.fc5(penultimate_output)\n        \n        if return_penultimate:\n            return penultimate_output\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Hyperparameters\ninput_dim = 3448  # Combined feature size\nhidden_dim = 512  # Hidden layer size\noutput_dim = 100  # Number of proxy labels (clusters)\nnum_epochs = 1\nbatch_size = 32\n\n# Initialize model, optimizer, and loss function\nmodel = VSJMNet(input_dim, hidden_dim, output_dim)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for features, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        # Move data to device\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        features, labels = features.to(device), labels.to(device)\n        model.to(device)\n        \n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class VSJMNet(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, output_dim):\n#         super(VSJMNet, self).__init__()\n#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n#         self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n#         self.fc4 = nn.Linear(hidden_dim, hidden_dim)  # Penultimate layer\n#         self.fc5 = nn.Linear(hidden_dim, output_dim)  # Final layer\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, x, return_penultimate=False):\n#         x = self.relu(self.fc1(x))\n#         x = self.dropout(x)\n#         x = self.relu(self.fc2(x))\n#         x = self.dropout(x)\n#         x = self.relu(self.fc3(x))\n#         x = self.dropout(x)\n#         penultimate_output = self.relu(self.fc4(x))\n#         x = self.fc5(penultimate_output)\n        \n#         if return_penultimate:\n#             return penultimate_output\n#         return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract embeddings\nmodel.eval()\npenultimate_activations = []\n\nwith torch.no_grad():\n    for features, _ in dataloader:\n        features = features.to(device)\n        embeddings = model(features, return_penultimate=True)\n        penultimate_activations.append(embeddings.cpu().numpy())\n\n# Save the activations\npenultimate_activations = np.vstack(penultimate_activations)\nnp.save(\"vsjm_joint_embeddings.npy\", penultimate_activations)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the saved embeddings\nvsjm_joint_embeddings = np.load(\"vsjm_joint_embeddings.npy\")\n\n# Print the shape of the embeddings\nprint(f\"VSJM Joint Embeddings shape: {vsjm_joint_embeddings.shape}\")\n\n# Print the embeddings (all or a subset for readability)\nprint(f\"VSJM Joint Embeddings: {vsjm_joint_embeddings}\")  # Full content\n\n# Optionally, print only the first 5 embeddings\nprint(f\"First 5 VSJM Joint Embeddings: {vsjm_joint_embeddings}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.move('vsjm_joint_embeddings.npy', '/kaggle/working/vsjm_joint_embeddings.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"shape of VJSM embeddings is (11 , 512) : Shape: This should show the shape of the saved embeddings, e.g., (num_samples, embedding_dim).","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Load the saved embeddings\nvsjm_joint_embeddings = np.load(\"vsjm_joint_embeddings.npy\")\n\n# Print all embeddings\nprint(\"VSJM Joint Embeddings (All Content):\")\nprint(vsjm_joint_embeddings)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Print embeddings in chunks of 5\n# for i in range(0, len(vsjm_joint_embeddings), 5):\n#     print(vsjm_joint_embeddings[i:i+5])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Updated Code to Save vsjm_joint_embeddings as a Dictionary","metadata":{}},{"cell_type":"code","source":"# Initialize a dictionary to store embeddings\nvsjm_joint_embeddings_dict = {}\n\nmodel.eval()\nwith torch.no_grad():\n    for i, (features, labels) in enumerate(dataloader):\n        # Move features to device\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        features = features.to(device)\n        \n        # Forward pass to extract penultimate layer embeddings\n        embeddings = model(features, return_penultimate=True).cpu().numpy()\n\n        \n        # Store embeddings in the dictionary with video IDs as keys\n        # batch_video_ids = [\n        #     list(ngram_to_video_standardized.values())[idx] \n        #     for idx in range(i * batch_size, i * batch_size + len(features))\n        # ]\n\n        max_index = len(train_features.keys())\n        batch_video_ids = [\n            list(train_features.keys())[idx]\n            for idx in range(i * batch_size, min(i * batch_size + len(features), max_index))\n        ]\n        \n        for video_id, embedding in zip(batch_video_ids, embeddings):\n            vsjm_joint_embeddings_dict[video_id] = embedding\n\n# Save the dictionary as a .npy file\nnp.save(\"vsjm_joint_embeddings_dict.npy\", vsjm_joint_embeddings_dict)\nprint(\"Saved embeddings as a dictionary with video IDs as keys.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.move('vsjm_joint_embeddings_dict.npy', '/kaggle/working/vsjm_joint_embeddings_dict.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vsjm_joint_embeddings = np.load(\"vsjm_joint_embeddings_dict.npy\", allow_pickle=True).item()\nprint(f\"Loaded {len(vsjm_joint_embeddings)} video embeddings.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the saved embeddings dictionary\nprint(\"VSJM Joint Embeddings Dictionary (Sample):\")\nfor video_id, embedding in list(vsjm_joint_embeddings_dict.items())[:5]:  # Print the first 5 entries\n    print(f\"Video ID: {video_id}, Embedding Shape: {embedding.shape}\")\n\n\n\n\n# # Print the saved embeddings dictionary\n# print(\"VSJM Joint Embeddings Dictionary (Sample):\")\n# for video_id, embedding in list(vsjm_joint_embeddings_dict.items())[:1]:  # Print the first 5 entries\n#     print(f\"Video ID: {video_id}, Embedding Shape: {embedding.shape}\")\n#     print(f\"Embedding Values: {embedding}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code to Print the Full Dictionary","metadata":{}},{"cell_type":"code","source":"# # Print the entire VSJM Joint Embeddings Dictionary\n# print(\"VSJM Joint Embeddings Dictionary (Full):\")\n# for video_id, embedding in vsjm_joint_embeddings.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"Embedding: {embedding}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"# Temporal Modeling with Hierarchical Descriptor Transformer (HDT):\n## The next step is : (Includes train and val datasets)\n\n\nDivide the VSJM Joint Embeddings into temporal chunks.\nPerform Hierarchical Fourier Transform or use another temporal refinement method to extract descriptors for further processing.","metadata":{}},{"cell_type":"markdown","source":"Implementation of Temporal Refinement","metadata":{}},{"cell_type":"markdown","source":"### Explanation of the Code\n\nDivide into Chunks:\nSplits frame-level embeddings (from VSJM-Net) into chunks of size chunk_size with an overlap of 50%.\nFourier Transform:\n\nApplies the Fast Fourier Transform (FFT) to each chunk to capture the frequency components.\nLow-Frequency Descriptors:\n\nRetains only low-frequency components (first cutoff frequencies) and aggregates them to form the temporal descriptors.\n\nOutput:\nRefined features for each video, where each chunk is represented by a temporal descriptor.","metadata":{}},{"cell_type":"markdown","source":"CODE to find \" Number of Temporal Chunks \" below :","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.fftpack import fft\n\n# Parameters for temporal refinement\nchunk_size = 64  # Number of frames per chunk\noverlap = 0.5  # 50% overlap\nlow_frequency_cutoff = 10  # Cutoff for low-frequency components\n\n# Function to divide embeddings into temporal chunks\ndef divide_into_chunks(features, chunk_size, overlap):\n    stride = int(chunk_size * (1 - overlap))\n    num_chunks = max(1, (len(features) - chunk_size) // stride + 1)\n    chunks = [features[i * stride: i * stride + chunk_size] for i in range(num_chunks)]\n    # print(\"chunk shape\" , chunks[0].shape) shape will be 64\n    return np.array(chunks)\n\n\n\n# Function to extract low-frequency temporal descriptors\ndef extract_low_frequency_descriptors(chunks, cutoff):\n    descriptors = []\n    for chunk in chunks:\n        # Apply Fourier Transform\n        freq_components = fft(chunk, axis=0)\n        low_freq = freq_components[:cutoff]  # Keep only low-frequency components\n        # descriptors.append(np.abs(low_freq).mean(axis=0))  # Aggregate low-frequency features\n        descriptors.append(np.abs(low_freq))  # Retain all low-frequency components\n    return np.array(descriptors)\n\n\n\n# Temporal Refinement Process\ndef temporal_refinement(vsjm_embeddings, chunk_size, overlap, low_frequency_cutoff):\n    refined_video_features = {}\n    for video_id, features in vsjm_embeddings.items():\n        # Divide features into chunks\n        chunks = divide_into_chunks(features, chunk_size, overlap)\n        # Extract low-frequency descriptors\n        descriptors = extract_low_frequency_descriptors(chunks, low_frequency_cutoff)\n        refined_video_features[video_id] = descriptors\n    return refined_video_features\n\n# Apply Temporal Refinement on VSJM embeddings\nprint(\"Performing temporal refinement on VSJM joint embeddings...\")\nrefined_features_train = temporal_refinement(vsjm_joint_embeddings_train_dict, chunk_size, overlap, low_frequency_cutoff)\nrefined_features_val = temporal_refinement(vsjm_joint_embeddings_val_dict, chunk_size, overlap, low_frequency_cutoff)\n\n\n\n# Example: Inspect refined features train\nsample_video_id_train = list(refined_features_train.keys())[0]\nprint(f\"Sample Video ID: {sample_video_id_train}\")\nprint(f\"Refined Feature Shape for {sample_video_id_train}: {refined_features_train[sample_video_id_train].shape}\")\n\n\n# Example: Inspect refined features val \nsample_video_id_val = list(refined_features_val.keys())[0]\nprint(f\"Sample Video ID: {sample_video_id_val}\")\nprint(f\"Refined Feature Shape for {sample_video_id_val}: {refined_features_val[sample_video_id_val].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# from scipy.fftpack import fft\n\n# # Parameters for temporal refinement\n# chunk_size = 64  # Number of frames per chunk\n# overlap = 0.5  # 50% overlap\n# low_frequency_cutoff = 10  # Cutoff for low-frequency components\n\n# # Function to divide embeddings into temporal chunks\n# def divide_into_chunks(features, chunk_size, overlap):\n#     stride = int(chunk_size * (1 - overlap))\n#     num_chunks = max(1, (len(features) - chunk_size) // stride + 1)\n#     chunks = [features[i * stride: i * stride + chunk_size] for i in range(num_chunks)]\n#     print(f\"Chunking: {len(features)} frames divided into {len(chunks)} chunks of size {chunk_size}.\")\n#     return np.array(chunks)\n\n# # Function to extract low-frequency temporal descriptors\n# def extract_low_frequency_descriptors(chunks, cutoff):\n#     descriptors = []\n#     for i, chunk in enumerate(chunks):\n#         # Apply Fourier Transform\n#         freq_components = fft(chunk, axis=0)\n#         low_freq = freq_components[:cutoff]  # Keep only low-frequency components\n#         descriptors.append(np.abs(low_freq))  # Retain all low-frequency components\n#         # print(f\"Chunk {i+1}: Fourier transform applied, descriptor shape: {low_freq.shape}\")\n#     return np.array(descriptors)\n\n# # Temporal Refinement Process\n# def temporal_refinement(vsjm_embeddings, chunk_size, overlap, low_frequency_cutoff):\n#     refined_video_features = {}\n#     for video_id, features in vsjm_embeddings.items():\n#         print(f\"\\nProcessing Video ID: {video_id}\")\n#         # Divide features into chunks\n#         chunks = divide_into_chunks(features, chunk_size, overlap)\n#         print(f\"Chunks shape for {video_id}: {chunks.shape}\")\n#         # Extract low-frequency descriptors\n#         descriptors = extract_low_frequency_descriptors(chunks, low_frequency_cutoff)\n#         print(f\"Low-frequency descriptors shape for {video_id}: {descriptors.shape}\")\n#         refined_video_features[video_id] = descriptors\n#     return refined_video_features\n\n# # Apply Temporal Refinement on VSJM embeddings\n# print(\"Performing temporal refinement on VSJM joint embeddings...\")\n# refined_features_train = temporal_refinement(vsjm_joint_embeddings_train_dict, chunk_size, overlap, low_frequency_cutoff)\n# refined_features_val = temporal_refinement(vsjm_joint_embeddings_val_dict, chunk_size, overlap, low_frequency_cutoff)\n\n# # Example: Inspect refined features train\n# sample_video_id_train = list(refined_features_train.keys())[0]\n# print(f\"\\nSample Video ID (Train): {sample_video_id_train}\")\n# print(f\"Refined Feature Shape for {sample_video_id_train}: {refined_features_train[sample_video_id_train].shape}\")\n\n# # Example: Inspect refined features val\n# sample_video_id_val = list(refined_features_val.keys())[0]\n# print(f\"\\nSample Video ID (Validation): {sample_video_id_val}\")\n# print(f\"Refined Feature Shape for {sample_video_id_val}: {refined_features_val[sample_video_id_val].shape}\")\n\n\n\n# # Function to check shapes of refined features\n# def check_refined_features_shapes(refined_features):\n#     for video_id, features in refined_features.items():\n#         print(f\"Video ID: {video_id}, Refined Feature Shape: {features.shape}\")\n\n# # Check shapes of refined features for training set\n# print(\"Shapes of Refined Features (Training Set):\")\n# check_refined_features_shapes(refined_features_train)\n\n# # Check shapes of refined features for validation set\n# print(\"\\nShapes of Refined Features (Validation Set):\")\n# check_refined_features_shapes(refined_features_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for video_id, refined in refined_features_train.items():\n#     print(f\"Video ID: {video_id}, Refined Shape: {refined.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(\"refined_video_features_train.npy\", refined_features_train)\nprint(\"Saved refined video features.\")\n\nnp.save(\"refined_video_features_val.npy\", refined_features_val)\nprint(\"Saved refined video features.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.move('refined_video_features_val.npy', '/kaggle/working/refined_video_features_val.npy')\nshutil.move('refined_video_features_train.npy', '/kaggle/working/refined_video_features_train.npy')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to compute number of temporal chunks for a video\ndef compute_number_of_chunks(video_features, chunk_size, overlap):\n    stride = int(chunk_size * (1 - overlap))\n    num_chunks = max(1, (len(video_features) - chunk_size) // stride + 1)\n    return num_chunks\n\n# Example: Compute number of temporal chunks for a sample video in train and val datasets\nsample_video_id_train = list(vsjm_joint_embeddings_train_dict.keys())[0]  # Replace with actual video ID\nsample_video_features_train = vsjm_joint_embeddings_train_dict[sample_video_id_train]\nnum_chunks_train = compute_number_of_chunks(sample_video_features_train, chunk_size, overlap)\nprint(f\"Number of Temporal Chunks for video in train data '{sample_video_id_train}': {num_chunks_train}\")\n\n\nsample_video_id_val = list(vsjm_joint_embeddings_val_dict.keys())[0]  # Replace with actual video ID\nsample_video_features_val = vsjm_joint_embeddings_val_dict[sample_video_id_val]\nnum_chunks_val = compute_number_of_chunks(sample_video_features_val, chunk_size, overlap)\nprint(f\"Number of Temporal Chunks for video in val data '{sample_video_id_val}': {num_chunks_val}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(f\"Number of keys in vsjm_joint_embeddings_val_dict: {len(vsjm_joint_embeddings_val_dict)}\")\n# print(f\"Keys: {list(vsjm_joint_embeddings_val_dict.keys())}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## print this refined_features","metadata":{}},{"cell_type":"markdown","source":"In the research paper, the **\"refined temporal features\"** (output from the **Hierarchical Descriptor Transformer (HDT)**) are described as:\r\n\r\n---\r\n\r\n### **Refined Temporal Features Dimension**\r\nThe refined temporal features represent the low-frequency temporal descriptors for each chunk of the video. The exact dimensions depend on the following:\r\n\r\n1. **Number of Temporal Chunks**:\r\n   - Determined by the length of the video, the chunk size, and the overlap.\r\n   - For a video divided into `N_chunks`, the refined features will have a temporal resolution equal to `N\n   - if shape of output from vsjm is 837 x 512 , then for each vid id , 512 is divided in 64 chuncks total of 15 chunks (50% overlap) and each will give 10 low freq so the output shape will be 837 X [15,10] of refined_features.\n\n     _chunks`.\r\n\r\n2. **Feature Dimension per Chunk**:\r\n   - Determined by the **low-frequency cutoff** applied during Fourier Transform.\r\n   - If the cutoff is set to 10 (as in your implementation), the output dimension for each chunk will be `D = 10`.\r\n\r\n3. **Final Output Dimension**:\r\n   - For a video with `N_chunks` chunks, the refined temporal features will have a shape of:\r\n     ```\r\n     [N_chunks, D]\r\n     ```\r\n     where:\r\n     - `N_chunks` = number of chunks the video is divided into.\r\n     - `D` = number of low-frequency components retained (e.g., 10).\r\n\r\n---\r\n\r\n### **Summary from Paper**\r\n- The refined temporal features have dimensions **[N_chunks, D]**.\r\n- This output is used as input to the **Video-Level Sequmy coder (VLSE)**.\r\n\r\n### **Your Current Implementation**\r\nIn your implementation:\r\n- `N_chunks` corresponds to the chunks derived by dividing the video.\r\n- `D` = 10, based on your `low_frequency_cutoff`.\r\n\r\nThe dimensions of your refined features `[N_chunks, 10]`.rating the output with the VLSE! ðŸš€","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Load the saved refined features\nprint(\"Loading refined video features of train datasets...\")\nrefined_features_train = np.load(\"refined_video_features_train.npy\", allow_pickle=True).item()\n\n\n# Print summary of the refined features\nprint(f\"Total number of videos in train : {len(refined_features_train)}\")\n\n\n# Inspect specific entries (e.g., the first 5 entries)\nfor idx, (video_id, features) in enumerate(refined_features_train.items()):\n    print(f\"Video ID: {video_id}\")\n    print(f\"Refined Feature Shape: {features.shape}\")\n    print(f\"Sample Feature Data:\\n{features}\\n\")\n    if idx == 0:  # Limit to first 5 entries\n        break\n\nprint(\"Loading refined video features of train datasets...\")\nrefined_features_val = np.load(\"refined_video_features_val.npy\", allow_pickle=True).item()\nprint(f\"Total number of videos in val: {len(refined_features_val)}\")\n\n\n# Inspect specific entries (e.g., the first 5 entries)\nfor idx, (video_id, features) in enumerate(refined_features_val.items()):\n    print(f\"Video ID: {video_id}\")\n    print(f\"Refined Feature Shape: {features.shape}\")\n    print(f\"Sample Feature Data:\\n{features}\\n\")\n    if idx == 0:  # Limit to first 5 entries\n        break        \n\n\n# # Optionally, inspect a specific video by ID\n# sample_video_id = list(refined_features.keys())[0]  # Replace with specific ID if known\n# print(f\"Inspecting refined features for video ID: {sample_video_id}\")\n# print(f\"Refined Feature Shape: {refined_features[sample_video_id].shape}\")\n# print(f\"Refined Features:\\n{refined_features[sample_video_id]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Next Steps\nOnce the temporal refinement is complete:\n\nSave the refined_features for use in the Proposal Generation phase.\nProceed with the Proposal Generation Network (VLSE) to detect temporal boundaries.\n","metadata":{}},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":"# Proposal Generation","metadata":{}},{"cell_type":"markdown","source":"1. Proposal Network (VLSE Implementation)\n\nThe Video-Level Sequence Encoder (VLSE) will process the refined temporal features from HDT using a Bidirectional LSTM to predict start and end scores for each event.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Define the Proposal Network using a Bidirectional LSTM\nclass ProposalNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(ProposalNetwork, self).__init__()\n        self.bilstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.fc_start = nn.Linear(hidden_dim * 2, 1)  # Start time scores\n        self.fc_end = nn.Linear(hidden_dim * 2, 1)    # End time scores\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Pass through BiLSTM\n        lstm_out, _ = self.bilstm(x)\n        # Predict start and end scores\n        start_scores = self.sigmoid(self.fc_start(lstm_out)).squeeze(-1)\n        end_scores = self.sigmoid(self.fc_end(lstm_out)).squeeze(-1)\n        return start_scores, end_scores\n\n\n# Note\n# Bidirectional LSTM Fusing Forward and Backward Scores\n# self.bilstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n\n\n\n# Define the Proposal Network using a Bidirectional LSTM\nclass ProposalNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(ProposalNetwork, self).__init__()\n        self.bilstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.fc_start = nn.Linear(hidden_dim * 2, 1)  # Start time scores\n        self.fc_end = nn.Linear(hidden_dim * 2, 1)    # End time scores\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Print the shape of input x\n        print(f\"Input x shape: {x.shape}\")\n\n        # Pass through BiLSTM\n        lstm_out, _ = self.bilstm(x)\n        print(f\"BiLSTM output shape: {lstm_out.shape}\")\n\n        # Predict start and end scores\n        start_scores = self.sigmoid(self.fc_start(lstm_out)).squeeze(-1)\n        end_scores = self.sigmoid(self.fc_end(lstm_out)).squeeze(-1)\n\n        # Print the shapes of start and end scores\n        print(f\"Start scores shape: {start_scores.shape}\")\n        print(f\"End scores shape: {end_scores.shape}\")\n\n        return start_scores, end_scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Prepare Data for Proposal Network\nThe refined features from HDT will be used as input. We will create a dataset to pass the features to the proposal network.","metadata":{}},{"cell_type":"code","source":"class ProposalDataset(Dataset):\n    def __init__(self, refined_features):\n        self.video_ids = list(refined_features.keys())\n        self.features = list(refined_features.values())\n        # print(len(video_ids))\n        print(features.shape)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        video_id = self.video_ids[idx]\n        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n        return video_id, feature\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load data into a DataLoader:","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset and dataloader for train \nprint(\"number of videos in train\",len(refined_features_train))\nprint(\"numberof videos in val\",len(refined_features_val))\n\nproposal_dataset_train = ProposalDataset(refined_features_train)\nprint(len(proposal_dataset_train))\nproposal_dataloader_train = DataLoader(proposal_dataset_train, batch_size=8, shuffle=True)\n\n# Prepare the dataset and dataloader for val \nproposal_dataset_val = ProposalDataset(refined_features_val)\nprint(len(proposal_dataset_val))\nproposal_dataloader_val = DataLoader(proposal_dataset_val, batch_size=8, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train Proposal Network","metadata":{}},{"cell_type":"code","source":"# Assuming trainval_data is already parsed from the annotation JSON file\n# trainval_data contains keys: \"video_id\", \"start_time\", \"end_time\", \"caption\", etc.\n\n# Extracting ground truth captions\nground_truth_captions = {}\nfor entry in trainval_data:\n    video_id = entry[\"video_id\"]\n    caption = entry[\"caption\"]\n\n    # Group captions by video ID\n    if video_id not in ground_truth_captions:\n        ground_truth_captions[video_id] = []\n    ground_truth_captions[video_id].append(caption)\n\n# Example: Check the content of ground_truth_captions\nprint(f\"Number of videos with captions: {len(ground_truth_captions)}\")\nsample_video_id = list(ground_truth_captions.keys())[0]\nprint(f\"Captions for video {sample_video_id}: {ground_truth_captions[sample_video_id]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor video_ids, features in proposal_dataloader_val:\n    print(f\"Batch Video IDs: {video_ids}\")\n    print(f\"Batch Feature Shape: {features.shape}\")\n\n\n# # Load the ground truth data\n# import json\n\n# with open(trainval_path, \"r\") as f:\n#     ground_truth_data = json.load(f)\n\n# # Dictionaries to store start and end times\n# ground_truth_start = {}\n# ground_truth_end = {}\n\n# # Extract start and end times for each video\n# for video_id, video_data in ground_truth_data[\"database\"].items():\n#     start_scores = []\n#     end_scores = []\n#     for annotation in video_data.get(\"annotations\", []):  # Access the annotations\n#         start_scores.append(annotation[\"segment\"][0])  # Extract the start time\n#         end_scores.append(annotation[\"segment\"][1])  # Extract the end time\n#     ground_truth_start[video_id] = torch.tensor(start_scores, dtype=torch.float32)\n#     ground_truth_end[video_id] = torch.tensor(end_scores, dtype=torch.float32)\n\n# # Filter for training and validation datasets\n# gt_start_train = {video_id: ground_truth_start[video_id] for video_id in refined_features_train.keys() if video_id in ground_truth_start}\n# gt_end_train = {video_id: ground_truth_end[video_id] for video_id in refined_features_train.keys() if video_id in ground_truth_end}\n\n# gt_start_val = {video_id: ground_truth_start[video_id] for video_id in refined_features_val.keys() if video_id in ground_truth_start}\n# gt_end_val = {video_id: ground_truth_end[video_id] for video_id in refined_features_val.keys() if video_id in ground_truth_end}\n\n# print(f\"Number of training video IDs with ground truth: {len(gt_start_train)}\")\n# print(f\"Number of validation video IDs with ground truth: {len(gt_start_val)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Iterate through the dataloader to inspect shapes and entries\nfor batch_idx, (video_ids, features) in enumerate(proposal_dataloader_train):\n    print(f\"Batch Index: {batch_idx}\")\n    print(f\"Video IDs: {video_ids[:8]}\")  # Print the first 8=batch size video IDs in the batch\n    print(f\"Features Shape: {features.shape}\")  # Print the shape of the features in the batch\n    \n    # Print a sample of the features\n    print(\"Sample Features (First Entry in Batch):\")\n    print(features[0])  # Print the first entry in the batch\n    \n    # Exit after inspecting the first batch\n    break\n\n\n\n# # Define model, loss function, and optimizer\n# input_dim = refined_features_train[sample_video_id_train].shape[1]  # Assuming the feature size is the same for train and val\n# hidden_dim = 128\n# model = ProposalNetwork(input_dim, hidden_dim)\n# criterion = nn.BCELoss()  # Binary Cross Entropy Loss for scores\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# # Combine training and validation features into datasets\n# train_keys, val_keys = list(refined_features_train.keys()), list(refined_features_val.keys())\n\n\n# # Training loop\n# num_epochs = 50\n# model.train()\n\n# for epoch in range(num_epochs):\n#     epoch_loss_train = 0.0\n#     epoch_loss_val = 0.0\n\n#     # Training phase\n#     model.train()\n#     for video_ids, features in proposal_dataloader_train:\n#         features = features.to(\"cuda\") if torch.cuda.is_available() else features\n#         model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n#         # Forward pass\n#         start_scores, end_scores = model(features)\n\n#         # Dummy ground truth (replace with real labels)\n#         gt_start = gt_start_train\n#         gt_end = gt_end_train\n\n#         # Compute loss\n#         loss_start = criterion(start_scores, gt_start)\n#         loss_end = criterion(end_scores, gt_end)\n#         loss = loss_start + loss_end\n\n#         # Backward pass and optimization\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         epoch_loss_train += loss.item()\n\n#     # Validation phase\n#     model.eval()\n#     with torch.no_grad():\n#         for video_ids, features in proposal_dataloader_val:\n#             features = features.to(\"cuda\") if torch.cuda.is_available() else features\n\n#             # Forward pass\n#             start_scores, end_scores = model(features)\n\n#             # Dummy ground truth (replace with real labels)\n#             gt_start = gt_end_val\n#             gt_end = gt_end_val\n\n#             # Compute loss\n#             loss_start = criterion(start_scores, gt_start)\n#             loss_end = criterion(end_scores, gt_end)\n#             loss = loss_start + loss_end\n\n#             epoch_loss_val += loss.item()\n\n#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss_train:.4f}, Validation Loss: {epoch_loss_val:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\n\n# Load the annotations\nannotations_path = \"/kaggle/input/annotations/youcookii_annotations_trainval/youcookii_annotations_trainval.json\"  # Replace with your path\nwith open(annotations_path, \"r\") as f:\n    annotations = json.load(f)[\"database\"]\n\n\ndef generate_ground_truth_batch(video_ids, annotations, num_chunks, chunk_size, video_duration):\n    \"\"\"\n    Generate ground truth for a batch of videos.\n\n    Args:\n        video_ids: List of video IDs in the batch.\n        annotations: Dictionary of video annotations.\n        num_chunks: Number of temporal chunks.\n        chunk_size: Number of frames per chunk.\n        video_duration: Duration of the video in seconds.\n\n    Returns:\n        gt_start_batch: Tensor of shape [batch_size, num_chunks] for start scores.\n        gt_end_batch: Tensor of shape [batch_size, num_chunks] for end scores.\n    \"\"\"\n    gt_start_batch = []\n    gt_end_batch = []\n\n    stride = video_duration / num_chunks  # Duration per chunk\n    for video_id in video_ids:\n        video_info = annotations[video_id]\n        start_times = [ann[\"segment\"][0] for ann in video_info[\"annotations\"]]\n        end_times = [ann[\"segment\"][1] for ann in video_info[\"annotations\"]]\n        \n        # Generate ground truth for the current video\n        gt_start = torch.zeros(num_chunks)\n        gt_end = torch.zeros(num_chunks)\n        for start, end in zip(start_times, end_times):\n            start_idx = int(start / stride)\n            end_idx = int(end / stride)\n            if start_idx < num_chunks:\n                gt_start[start_idx] = 1\n            if end_idx < num_chunks:\n                gt_end[end_idx] = 1\n\n        gt_start_batch.append(gt_start)\n        gt_end_batch.append(gt_end)\n\n    return torch.stack(gt_start_batch), torch.stack(gt_end_batch)\n\n\ndef fetch_video_annotations(annotations, video_id):\n    \"\"\"\n    Fetch the duration, start, and end times from annotations for a given video ID.\n\n    Args:\n        annotations: Loaded JSON annotations.\n        video_id: Video ID for which to fetch data.\n\n    Returns:\n        video_duration: Duration of the video.\n        start_times: List of start times for segments.\n        end_times: List of end times for segments.\n    \"\"\"\n    # Assuming `annotations` directly contains video IDs as keys\n    if video_id not in annotations:\n        raise KeyError(f\"Video ID '{video_id}' not found in annotations.\")\n\n    video_info = annotations[video_id]\n    video_duration = video_info[\"duration\"]\n    start_times = [segment[\"segment\"][0] for segment in video_info[\"annotations\"]]\n    end_times = [segment[\"segment\"][1] for segment in video_info[\"annotations\"]]\n\n    return video_duration, start_times, end_times\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define model, loss function, and optimizer\ninput_dim = refined_features_train[sample_video_id_train].shape[1]  # Assuming the feature size is the same for train and val\nprint(\"Input dim: \", input_dim)\nhidden_dim = 128\n\nmodel = ProposalNetwork(input_dim, hidden_dim)\n\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss for scores\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\n# Combine training and validation features into datasets\ntrain_keys, val_keys = list(refined_features_train.keys()), list(refined_features_val.keys())\n\n\n# Training loop\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    epoch_loss_train = 0.0\n    epoch_loss_val = 0.0\n\n    # Training phase\n    model.train()\n    for video_ids, features in proposal_dataloader_train:\n        features = features.to(\"cuda\") if torch.cuda.is_available() else features\n        model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n\n        # Fetch duration and ground truth start/end times from annotations\n        video_id = video_ids[0]  # Assuming batch size of 1 for simplicity\n        video_duration, start_times, end_times = fetch_video_annotations(annotations, video_id)\n        print(\"id:\", video_id , \"::\", \"Duration:\", video_duration)\n\n        \n        # Forward pass\n        start_scores, end_scores = model(features)\n        # print(\"start score:\",start_scores )\n        # print(\"end score:\",end_scores )\n        \n\n        # Dummy ground truth (replace with real labels)\n        # gt_start = torch.zeros_like(start_scores)\n        # gt_end = torch.ones_like(end_scores)\n\n        # Generate ground truth for the batch\n        num_chunks = features.shape[1]\n        # video_duration = 10.0  # Replace with actual duration for each video\n        gt_start, gt_end = generate_ground_truth_batch(\n            video_ids, annotations, num_chunks, chunk_size, video_duration\n        )\n        gt_start = gt_start.to(\"cuda\") if torch.cuda.is_available() else gt_start\n        gt_end = gt_end.to(\"cuda\") if torch.cuda.is_available() else gt_end\n        # print(\"gt_start:\", gt_start)\n        # print(\"gt_start:\", gt_start.shape)\n    \n\n        # Compute loss\n        loss_start = criterion(start_scores, gt_start)\n        loss_end = criterion(end_scores, gt_end)\n        loss = loss_start + loss_end\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss_train += loss.item()\n\n    # Validation phase\n    model.eval()\n    with torch.no_grad():\n        for video_ids, features in proposal_dataloader_val:\n            features = features.to(\"cuda\") if torch.cuda.is_available() else features\n\n            # Forward pass\n            start_scores, end_scores = model(features)\n\n            # Dummy ground truth (replace with real labels)\n            gt_start = torch.zeros_like(start_scores)\n            gt_end = torch.ones_like(end_scores)\n\n            # Compute loss\n            loss_start = criterion(start_scores, gt_start)\n            loss_end = criterion(end_scores, gt_end)\n            loss = loss_start + loss_end\n\n            epoch_loss_val += loss.item()\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss_train:.4f}, Validation Loss: {epoch_loss_val:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Define model, loss function, and optimizer\n# input_dim = refined_features_val[sample_video_id_val].shape[1]  # e.g., 10\n# hidden_dim = 128\n# model = ProposalNetwork(input_dim, hidden_dim)\n# criterion = nn.BCELoss()  # Binary Cross Entropy Loss for scores\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# # Training loop\n# num_epochs = 50\n# model.train()\n\n# for epoch in range(num_epochs):\n#     epoch_loss = 0.0\n#     for video_ids, features in proposal_dataloader:\n#         # Forward pass\n#         start_scores, end_scores = model(features)\n\n#         # Dummy ground truth (for example purposes, replace with real labels)\n#         gt_start = torch.zeros_like(start_scores)\n#         gt_end = torch.ones_like(end_scores)\n\n#         # Compute loss\n#         loss_start = criterion(start_scores, gt_start)\n#         loss_end = criterion(end_scores, gt_end)\n#         loss = loss_start + loss_end\n\n#         # Backward pass and optimization\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         epoch_loss += loss.item()\n\n#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Output Proposals\nGenerate temporal event proposals using the trained model.\n\npython\nCopy code\n","metadata":{}},{"cell_type":"code","source":"model.eval()\nproposals_train = {}\nproposals_val = {}\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Process training features\nprint(\"Generating proposals for training data...\")\nwith torch.no_grad():\n    for video_ids, features in proposal_dataloader_train:\n        features = features.to(device)\n        start_scores, end_scores = model(features)\n        \n        # Generate proposals for training data\n        for video_id, start, end in zip(video_ids, start_scores, end_scores):\n            proposals_train[video_id] = {\n                \"start\": start.cpu().tolist(),\n                \"end\": end.cpu().tolist(),\n                \"confidence\": (start + end).cpu().tolist()  # Example confidence metric\n            }\n            \n\nprint(f\"Generated proposals for {len(proposals_train)} training videos.\")\n\n# Process validation features\nprint(\"Generating proposals for validation data...\")\nwith torch.no_grad():\n    for video_ids, features in proposal_dataloader_val:\n        features = features.to(device)\n        start_scores, end_scores = model(features)\n        \n        # Generate proposals for validation data\n        for video_id, start, end in zip(video_ids, start_scores, end_scores):\n            proposals_val[video_id] = {\n                \"start\": start.cpu().tolist(),\n                \"end\": end.cpu().tolist(),\n                \"confidence\": (start + end).cpu().tolist()  # Example confidence metric\n            }\n\nprint(f\"Generated proposals for {len(proposals_val)} validation videos.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Print a sample of proposals from proposals_train\n# print(\"Proposals for Training Data (Sample):\")\n# for video_id, proposal in list(proposals_train.items())[:2]:  # Adjust the slice [:5] to print more or fewer samples\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Start Scores shape: {len(proposal['start'])}\")\n#     print(f\"  Start Scores: {proposal['start'][:10]}\")  # Print only the first 10 start scores for brevity\n#     print(f\"  End Scores shape: {len(proposal['start'])}\")\n#     print(f\"  End Scores: {proposal['end'][:10]}\")      # Print only the first 10 end scores for brevity\n#     print(f\"  Confidence Scores: {proposal['confidence'][:10]}\")  # Print only the first 10 confidence scores for brevity\")\n#     print(\"-\" * 50)  # Separator for readability\n\n\n\n\n# Print a sample of proposals from proposals_train\n# Check the number of entries in proposals_train\nnum_entries = len(proposals_train)\nprint(f\"Number of entries in proposals_train: {num_entries}\")\n\nprint(\"Proposals for Training Data (Sample):\")\nfor video_id, proposal in list(proposals_train.items())[:2]:  # Adjust the slice [:2] to print more or fewer samples\n    print(f\"Video ID: {video_id}\")\n    \n    # Print shapes\n    print(f\"  Shape of Start Scores: {len(proposal['start'])}\")  # Length of start scores\n    print(f\"  Shape of End Scores: {len(proposal['end'])}\")      # Length of end scores\n    print(f\"  Shape of Confidence Scores: {len(proposal['confidence'])}\")  # Length of confidence scores\n    \n    # Print values\n    print(f\"  Start Scores: {proposal['start'][:10]}\")  # Print only the first 10 start scores for brevity\n    print(f\"  End Scores: {proposal['end'][:10]}\")      # Print only the first 10 end scores for brevity\n    print(f\"  Confidence Scores: {proposal['confidence'][:10]}\")  # Print only the first 10 confidence scores for brevity\n    print(\"-\" * 50)  # Separator for readability\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# proposals = {}\n\n# with torch.no_grad():\n#     for video_ids, features in proposal_dataloader:\n#         start_scores, end_scores = model(features)\n        \n#         # Generate proposals (replace with more robust thresholds)\n#         for video_id, start, end in zip(video_ids, start_scores, end_scores):\n#             proposals[video_id] = {\n#                 \"start\": start.tolist(),\n#                 \"end\": end.tolist(),\n#                 \"confidence\": (start + end).tolist()  # Example confidence metric\n#             }\n\n# print(f\"Generated proposals for {len(proposals)} videos.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Output from VLSE\nPurpose of VLSE:\n\nThe VLSE is responsible for detecting the start times, end times, and associated confidence scores for temporal event proposals in a video.\nOutput Dimensions:\n\nThe output for each video typically consists of:\nStart Scores: A sequence of scores for each temporal chunk indicating the likelihood of it being a start of an event.\nEnd Scores: A sequence of scores for each temporal chunk indicating the likelihood of it being the end of an event.\nConfidence Scores: A measure of confidence for the detected start and end boundaries.\nSpecific Dimensions:\n\nIf a video has N_chunks temporal chunks and each chunk has a D_dimensional refined feature, then:\nStart Scores: [N_chunks] (1 score per chunk)\nEnd Scores: [N_chunks] (1 score per chunk)\nConfidence Scores: [N_chunks] (can combine start and end scores, e.g., via summation or averaging).","metadata":{}},{"cell_type":"code","source":"# # Print dimensions of proposals\n# for video_id, proposal in proposals.items():\n#     start_dim = len(proposal['start'])\n#     end_dim = len(proposal['end'])\n#     confidence_dim = len(proposal['confidence'])\n    \n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Dimension of Start Scores: {start_dim}\")\n#     print(f\"  Dimension of End Scores: {end_dim}\")\n#     print(f\"  Dimension of Confidence: {confidence_dim}\\n\")\n\n# # Print proposals in a detailed format\n# for video_id, proposal in proposals.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Start Scores: {proposal['start']}\")\n#     print(f\"  End Scores: {proposal['end']}\")\n#     print(f\"  Confidence: {proposal['confidence']}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"For Train dataset....\")\n# # Print dimensions of proposals for train\n# for video_id, proposal in proposals_train.items():\n#     start_dim = len(proposal['start'])\n#     end_dim = len(proposal['end'])\n#     confidence_dim = len(proposal['confidence'])\n    \n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Dimension of Start Scores: {start_dim}\")\n#     print(f\"  Dimension of End Scores: {end_dim}\")\n#     print(f\"  Dimension of Confidence: {confidence_dim}\\n\")\n\n# # Print proposals in a detailed format for train \n# for video_id, proposal in proposals_train.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Start Scores: {proposal['start']}\")\n#     print(f\"  End Scores: {proposal['end']}\")\n#     print(f\"  Confidence: {proposal['confidence']}\\n\")\n\n\n# print(\"For Val dataset....\")\n# # Print dimensions of proposals for train\n# for video_id, proposal in proposals_val.items():\n#     start_dim = len(proposal['start'])\n#     end_dim = len(proposal['end'])\n#     confidence_dim = len(proposal['confidence'])\n    \n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Dimension of Start Scores: {start_dim}\")\n#     print(f\"  Dimension of End Scores: {end_dim}\")\n#     print(f\"  Dimension of Confidence: {confidence_dim}\\n\")\n\n# # Print proposals in a detailed format for train \n# for video_id, proposal in proposals_val.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(f\"  Start Scores: {proposal['start']}\")\n#     print(f\"  End Scores: {proposal['end']}\")\n#     print(f\"  Confidence: {proposal['confidence']}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"filter proposals based on a confidence threshold (e.g., tIoU â‰¥ 0.8) and saving the results for inspection.","metadata":{}},{"cell_type":"code","source":"# # Confidence threshold for filtering\n# confidence_threshold = 0.8\n\n# # Function to filter proposals\n# def filter_proposals(proposals, confidence_threshold):\n#     filtered_proposals = {}\n#     for video_id, proposal in proposals.items():\n#         # Filter based on confidence\n#         start_scores = proposal[\"start\"]\n#         end_scores = proposal[\"end\"]\n#         confidence_scores = proposal[\"confidence\"]\n\n#         valid_proposals = []\n#         for start, end, conf in zip(start_scores, end_scores, confidence_scores):\n#             if conf >= confidence_threshold:\n#                 valid_proposals.append({\"start\": start, \"end\": end, \"confidence\": conf})\n\n#         # Only store filtered proposals if any are valid\n#         if valid_proposals:\n#             filtered_proposals[video_id] = valid_proposals\n\n#     return filtered_proposals\n\n# # Apply filtering\n# print(\"Filtering proposals based on confidence threshold...\")\n# filtered_proposals = filter_proposals(proposals, confidence_threshold)\n# print(f\"Filtered proposals for {len(filtered_proposals)} videos.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confidence threshold for filtering\nconfidence_threshold = 0.85\n\n# Function to filter proposals\ndef filter_proposals(proposals, confidence_threshold):\n    filtered_proposals = {}\n    for video_id, proposal in proposals.items():\n        # Extract proposal details\n        start_scores = proposal[\"start\"]\n        end_scores = proposal[\"end\"]\n        confidence_scores = proposal[\"confidence\"]\n\n        # Filter based on confidence\n        valid_proposals = []\n        for start, end, conf in zip(start_scores, end_scores, confidence_scores):\n            if conf >= confidence_threshold:\n                valid_proposals.append({\"start\": start, \"end\": end, \"confidence\": conf})\n\n        # Store only valid proposals\n        if valid_proposals:\n            filtered_proposals[video_id] = valid_proposals\n\n    return filtered_proposals\n\n# Apply filtering for training proposals\nprint(\"Filtering training proposals based on confidence threshold...\")\nfiltered_proposals_train = filter_proposals(proposals_train, confidence_threshold)\nprint(f\"Filtered training proposals for {len(filtered_proposals_train)} videos.\")\n\n# Apply filtering for validation proposals\nfiltered_proposals_val = filter_proposals(proposals_val, confidence_threshold)\nprint(f\"Filtered validation proposals for {len(filtered_proposals_val)} videos.\")\n\n# # Optionally save the filtered proposals\n# np.save(\"filtered_proposals_train.npy\", filtered_proposals_train)\n# np.save(\"filtered_proposals_val.npy\", filtered_proposals_val)\n\n# print(\"Filtered proposals have been saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:20.864272Z","iopub.execute_input":"2025-01-04T17:30:20.864641Z","iopub.status.idle":"2025-01-04T17:30:20.874497Z","shell.execute_reply.started":"2025-01-04T17:30:20.864610Z","shell.execute_reply":"2025-01-04T17:30:20.873437Z"}},"outputs":[{"name":"stdout","text":"Filtering training proposals based on confidence threshold...\nFiltered training proposals for 284 videos.\nFiltered validation proposals for 167 videos.\n","output_type":"stream"}],"execution_count":190},{"cell_type":"markdown","source":"Save the filtered proposals to a JSON file for inspection.","metadata":{}},{"cell_type":"code","source":"# import json\n\n# # Save filtered proposals to a file\n# filtered_proposals_path = \"filtered_video_proposals.json\"\n# with open(filtered_proposals_path, \"w\") as f:\n#     json.dump(filtered_proposals, f, indent=4)\n# print(f\"Filtered proposals saved to {filtered_proposals_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:23.185918Z","iopub.execute_input":"2025-01-04T17:30:23.186253Z","iopub.status.idle":"2025-01-04T17:30:23.190497Z","shell.execute_reply.started":"2025-01-04T17:30:23.186224Z","shell.execute_reply":"2025-01-04T17:30:23.189527Z"}},"outputs":[],"execution_count":191},{"cell_type":"code","source":"import json\n\n# Save filtered training proposals to a file\nfiltered_proposals_train_path = \"filtered_training_proposals.json\"\nwith open(filtered_proposals_train_path, \"w\") as f:\n    json.dump(filtered_proposals_train, f, indent=4)\nprint(f\"Filtered training proposals saved to {filtered_proposals_train_path}.\")\n\n# Save filtered validation proposals to a file\nfiltered_proposals_val_path = \"filtered_validation_proposals.json\"\nwith open(filtered_proposals_val_path, \"w\") as f:\n    json.dump(filtered_proposals_val, f, indent=4)\nprint(f\"Filtered validation proposals saved to {filtered_proposals_val_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:23.498439Z","iopub.execute_input":"2025-01-04T17:30:23.498738Z","iopub.status.idle":"2025-01-04T17:30:23.513219Z","shell.execute_reply.started":"2025-01-04T17:30:23.498713Z","shell.execute_reply":"2025-01-04T17:30:23.512363Z"}},"outputs":[{"name":"stdout","text":"Filtered training proposals saved to filtered_training_proposals.json.\nFiltered validation proposals saved to filtered_validation_proposals.json.\n","output_type":"stream"}],"execution_count":192},{"cell_type":"markdown","source":"To inspect the saved proposals:","metadata":{}},{"cell_type":"code","source":"# # Load and inspect saved proposals\n# with open(filtered_proposals_path, \"r\") as f:\n#     loaded_proposals = json.load(f)\n\n# # Print summary\n# for video_id, proposal_list in list(loaded_proposals.items())[:5]:  # Print first 5 videos\n#     print(f\"Video ID: {video_id}\")\n#     for proposal in proposal_list:\n#         print(f\"  Start: {proposal['start']}, End: {proposal['end']}, Confidence: {proposal['confidence']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:29.640784Z","iopub.execute_input":"2025-01-04T17:30:29.641102Z","iopub.status.idle":"2025-01-04T17:30:29.645212Z","shell.execute_reply.started":"2025-01-04T17:30:29.641076Z","shell.execute_reply":"2025-01-04T17:30:29.644269Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"import json\n\n# # Load and inspect saved training proposals\n# with open(\"filtered_training_proposals.json\", \"r\") as f:\n#     loaded_proposals_train = json.load(f)\n\n# Print summary for training proposals\nprint(\"Training Proposals (First 5 Videos):\")\nfor video_id, proposal_list in list(loaded_proposals_train.items())[:5]:  # Print first 5 videos\n    print(f\"Video ID: {video_id}\")\n    for proposal in proposal_list:\n        print(f\"  Start: {proposal['start']}, End: {proposal['end']}, Confidence: {proposal['confidence']}\")\n\n# # Load and inspect saved validation proposals\n# with open(\"filtered_validation_proposals.json\", \"r\") as f:\n#     loaded_proposals_val = json.load(f)\n\n# Print summary for validation proposals\nprint(\"\\nValidation Proposals (First 5 Videos):\")\nfor video_id, proposal_list in list(loaded_proposals_val.items())[:5]:  # Print first 5 videos\n    print(f\"Video ID: {video_id}\")\n    for proposal in proposal_list:\n        print(f\"  Start: {proposal['start']}, End: {proposal['end']}, Confidence: {proposal['confidence']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:29.993913Z","iopub.execute_input":"2025-01-04T17:30:29.994156Z","iopub.status.idle":"2025-01-04T17:30:30.000687Z","shell.execute_reply.started":"2025-01-04T17:30:29.994133Z","shell.execute_reply":"2025-01-04T17:30:29.999667Z"}},"outputs":[{"name":"stdout","text":"Training Proposals (First 5 Videos):\nVideo ID: Ck3oNHsLqfk\n  Start: 0.46365225315093994, End: 0.3868759572505951, Confidence: 0.8505282402038574\nVideo ID: nfO0YS2dSdw\n  Start: 0.49565061926841736, End: 0.45762962102890015, Confidence: 0.9532802104949951\n  Start: 0.4297132194042206, End: 0.4447507858276367, Confidence: 0.8744640350341797\n  Start: 0.4256313443183899, End: 0.4500196874141693, Confidence: 0.8756510019302368\nVideo ID: EA4EWlv6HUM\n  Start: 0.46647951006889343, End: 0.3929535746574402, Confidence: 0.8594330549240112\nVideo ID: L8ZRBTdq3mE\n  Start: 0.49486109614372253, End: 0.45816749334335327, Confidence: 0.9530285596847534\n  Start: 0.42855104804039, End: 0.4447736442089081, Confidence: 0.8733246922492981\n  Start: 0.42501506209373474, End: 0.45097804069519043, Confidence: 0.8759931325912476\nVideo ID: _xOx9hkJoBk\n  Start: 0.46371376514434814, End: 0.3871971070766449, Confidence: 0.8509109020233154\n\nValidation Proposals (First 5 Videos):\nVideo ID: Amc5E5Pmqyc\n  Start: 0.4723258912563324, End: 0.4039705693721771, Confidence: 0.8762964606285095\nVideo ID: bQ3QYyFglc0\n  Start: 0.47086769342422485, End: 0.39682427048683167, Confidence: 0.8676919937133789\nVideo ID: xfiIOUS6FpQ\n  Start: 0.4721800982952118, End: 0.40199577808380127, Confidence: 0.8741759061813354\nVideo ID: ID2ct1TlVuk\n  Start: 0.4667685627937317, End: 0.39423975348472595, Confidence: 0.8610082864761353\nVideo ID: P71govkTh1E\n  Start: 0.46733754873275757, End: 0.3955961763858795, Confidence: 0.8629337549209595\n","output_type":"stream"}],"execution_count":194},{"cell_type":"markdown","source":"proposals details with its dimension ","metadata":{}},{"cell_type":"code","source":"# # Load the filtered proposals file\n# with open(\"filtered_video_proposals.json\", \"r\") as f:\n#     filtered_proposals = json.load(f)\n\n# # Print the dimensions of filtered proposals\n# print(\"Dimensions of Filtered Proposals:\")\n# for video_id, proposals in filtered_proposals.items():\n#     print(f\"Video ID: {video_id}, Number of Proposals: {len(proposals)}\")\n#     for idx, proposal in enumerate(proposals):\n#         print(f\"  Proposal {idx + 1}: Start={proposal['start']}, End={proposal['end']}, Confidence={proposal['confidence']}\")\n#     print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:32.850953Z","iopub.execute_input":"2025-01-04T17:30:32.851978Z","iopub.status.idle":"2025-01-04T17:30:32.855603Z","shell.execute_reply.started":"2025-01-04T17:30:32.851941Z","shell.execute_reply":"2025-01-04T17:30:32.854798Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"import json\n\n# # Load the filtered training proposals file\n# with open(\"filtered_training_proposals.json\", \"r\") as f:\n#     filtered_proposals_train = json.load(f)\n\n# # Load the filtered validation proposals file\n# with open(\"filtered_validation_proposals.json\", \"r\") as f:\n#     filtered_proposals_val = json.load(f)\n\n# Function to print dimensions of proposals\ndef print_proposal_dimensions(proposals, dataset_name):\n    print(f\"\\nDimensions of Filtered Proposals for {dataset_name}:\")\n    for video_id, proposal_list in list(proposals.items())[:5]:  # Limit to first 5 videos for readability\n        print(f\"Video ID: {video_id}, Number of Proposals: {len(proposal_list)}\")\n        for idx, proposal in enumerate(proposal_list):\n            print(f\"  Proposal {idx + 1}: Start={proposal['start']}, End={proposal['end']}, Confidence={proposal['confidence']:.4f}\")\n        print(\"-\" * 50)\n\n# Print dimensions for training proposals\nprint_proposal_dimensions(filtered_proposals_train, \"Training\")\n\n# Print dimensions for validation proposals\nprint_proposal_dimensions(filtered_proposals_val, \"Validation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:33.476999Z","iopub.execute_input":"2025-01-04T17:30:33.477656Z","iopub.status.idle":"2025-01-04T17:30:33.484059Z","shell.execute_reply.started":"2025-01-04T17:30:33.477624Z","shell.execute_reply":"2025-01-04T17:30:33.483073Z"}},"outputs":[{"name":"stdout","text":"\nDimensions of Filtered Proposals for Training:\nVideo ID: Ck3oNHsLqfk, Number of Proposals: 1\n  Proposal 1: Start=0.46365225315093994, End=0.3868759572505951, Confidence=0.8505\n--------------------------------------------------\nVideo ID: nfO0YS2dSdw, Number of Proposals: 3\n  Proposal 1: Start=0.49565061926841736, End=0.45762962102890015, Confidence=0.9533\n  Proposal 2: Start=0.4297132194042206, End=0.4447507858276367, Confidence=0.8745\n  Proposal 3: Start=0.4256313443183899, End=0.4500196874141693, Confidence=0.8757\n--------------------------------------------------\nVideo ID: EA4EWlv6HUM, Number of Proposals: 1\n  Proposal 1: Start=0.46647951006889343, End=0.3929535746574402, Confidence=0.8594\n--------------------------------------------------\nVideo ID: L8ZRBTdq3mE, Number of Proposals: 3\n  Proposal 1: Start=0.49486109614372253, End=0.45816749334335327, Confidence=0.9530\n  Proposal 2: Start=0.42855104804039, End=0.4447736442089081, Confidence=0.8733\n  Proposal 3: Start=0.42501506209373474, End=0.45097804069519043, Confidence=0.8760\n--------------------------------------------------\nVideo ID: _xOx9hkJoBk, Number of Proposals: 1\n  Proposal 1: Start=0.46371376514434814, End=0.3871971070766449, Confidence=0.8509\n--------------------------------------------------\n\nDimensions of Filtered Proposals for Validation:\nVideo ID: Amc5E5Pmqyc, Number of Proposals: 1\n  Proposal 1: Start=0.4723258912563324, End=0.4039705693721771, Confidence=0.8763\n--------------------------------------------------\nVideo ID: bQ3QYyFglc0, Number of Proposals: 1\n  Proposal 1: Start=0.47086769342422485, End=0.39682427048683167, Confidence=0.8677\n--------------------------------------------------\nVideo ID: xfiIOUS6FpQ, Number of Proposals: 1\n  Proposal 1: Start=0.4721800982952118, End=0.40199577808380127, Confidence=0.8742\n--------------------------------------------------\nVideo ID: ID2ct1TlVuk, Number of Proposals: 1\n  Proposal 1: Start=0.4667685627937317, End=0.39423975348472595, Confidence=0.8610\n--------------------------------------------------\nVideo ID: P71govkTh1E, Number of Proposals: 1\n  Proposal 1: Start=0.46733754873275757, End=0.3955961763858795, Confidence=0.8629\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":196},{"cell_type":"markdown","source":"## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@","metadata":{}},{"cell_type":"markdown","source":" # Caption Generation","metadata":{}},{"cell_type":"code","source":"# Assuming trainval_data is already parsed from the annotation JSON file\n# trainval_data contains keys: \"video_id\", \"start_time\", \"end_time\", \"caption\", etc.\n\n# Extracting ground truth captions\nground_truth_captions = {}\nfor entry in trainval_data:\n    video_id = entry[\"video_id\"]\n    caption = entry[\"caption\"]\n\n    # Group captions by video ID\n    if video_id not in ground_truth_captions:\n        ground_truth_captions[video_id] = []\n    ground_truth_captions[video_id].append(caption)\n\n# Example: Check the content of ground_truth_captions\nprint(f\"Number of videos with captions: {len(ground_truth_captions)}\")\nsample_video_id = list(ground_truth_captions.keys())[0]\nprint(f\"Captions for video {sample_video_id}: {ground_truth_captions[sample_video_id]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:30:36.552368Z","iopub.execute_input":"2025-01-04T17:30:36.552743Z","iopub.status.idle":"2025-01-04T17:30:36.564772Z","shell.execute_reply.started":"2025-01-04T17:30:36.552713Z","shell.execute_reply":"2025-01-04T17:30:36.563770Z"}},"outputs":[{"name":"stdout","text":"Number of videos with captions: 1790\nCaptions for video GLd3aX16zBg: ['spread margarine on two slices of white bread', 'place a slice of cheese on the bread', 'place the bread slices on top of each other and place in a hot pan', 'flip the sandwich over and press down', 'cut the sandwich in half diagonally']\n","output_type":"stream"}],"execution_count":197},{"cell_type":"markdown","source":"## code start from here ------------------ to check ground_truth_captions AND refined_features allignment","metadata":{}},{"cell_type":"markdown","source":"Check the keys in both refined_features and ground_truth_captions to ensure they align.","metadata":{}},{"cell_type":"code","source":"# print(f\"Sample keys from refined_features: {list(refined_features.keys())[:5]}\")\n# print(f\"Sample keys from ground_truth_captions: {list(ground_truth_captions.keys())[:5]}\")\n\n# # Check the intersection\n# aligned_keys = set(refined_features.keys()) & set(ground_truth_captions.keys())\n# print(f\"Number of aligned keys: {len(aligned_keys)}\")\n\n# Print sample keys from refined features and ground truth captions\nprint(\"\\nInspecting Refined Features and Ground Truth Captions:\")\nprint(f\"Sample Keys from Refined Features (Training): {list(refined_features_train.keys())[:5]}\")\nprint(f\"Sample Keys from Refined Features (Validation): {list(refined_features_val.keys())[:5]}\")\nprint(f\"Sample Keys from Ground Truth Captions: {list(ground_truth_captions.keys())[:5]}\")\n\n# Check intersections for training and validation\naligned_keys_train = set(refined_features_train.keys()) & set(ground_truth_captions.keys())\naligned_keys_val = set(refined_features_val.keys()) & set(ground_truth_captions.keys())\n\n# Print aligned key statistics\nprint(\"\\nAligned Key Statistics:\")\nprint(f\"Number of Aligned Keys (Training): {len(aligned_keys_train)}\")\nprint(f\"Number of Aligned Keys (Validation): {len(aligned_keys_val)}\")\n\n# Debug aligned keys (optional, limit to first few keys for readability)\nprint(\"\\nSample Aligned Keys (Training):\", list(aligned_keys_train)[:5])\nprint(\"Sample Aligned Keys (Validation):\", list(aligned_keys_val)[:5])\nprint(\"Sample Ground Truth Keys:\", list(ground_truth_captions)[:5])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:35:34.999379Z","iopub.execute_input":"2025-01-04T17:35:35.000332Z","iopub.status.idle":"2025-01-04T17:35:35.010192Z","shell.execute_reply.started":"2025-01-04T17:35:35.000286Z","shell.execute_reply":"2025-01-04T17:35:35.009163Z"}},"outputs":[{"name":"stdout","text":"\nInspecting Refined Features and Ground Truth Captions:\nSample Keys from Refined Features (Training): ['LoeSv5enisE', 'v7U70gPylBA', 'XeXQ23fgS38', 'sb58mkT8_ak', 'A4pTxrrBtMU']\nSample Keys from Refined Features (Validation): ['arQZ2m0hMSU', 'OsYU9W14guU', 'bqmkNRNd5yw', 'OOX012L_cXA', 'HLQ16akzeI8']\nSample Keys from Ground Truth Captions: ['GLd3aX16zBg', 'xHr8X2Wpmno', 'V53XmPeyjIU', 'mZwK0TBI1iY', 'H_9oM1Y60lU']\n\nAligned Key Statistics:\nNumber of Aligned Keys (Training): 669\nNumber of Aligned Keys (Validation): 168\n\nSample Aligned Keys (Training): ['Epl4sPuetO0', '_HYoidFnb5w', 'FfyOV00sXEY', 'w7jJ8nsbbKI', '9K3HfJqsY-M']\nSample Aligned Keys (Validation): ['G-vY8T7pSTM', 'u2U7VkXHtMU', 'kSyFQ1hWBLU', 'S4z2gmtUzHE', 'ETXVbDoMKbQ']\nSample Ground Truth Keys: ['GLd3aX16zBg', 'xHr8X2Wpmno', 'V53XmPeyjIU', 'mZwK0TBI1iY', 'H_9oM1Y60lU']\n","output_type":"stream"}],"execution_count":200},{"cell_type":"markdown","source":"Inspect ground_truth_captions:","metadata":{}},{"cell_type":"code","source":"# # Inspect a few entries from ground_truth_captions\n# for key, value in list(ground_truth_captions.items())[:5]:\n#     print(f\"Key: {key}, Value: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:04:08.974582Z","iopub.execute_input":"2025-01-04T17:04:08.975036Z","iopub.status.idle":"2025-01-04T17:04:08.979282Z","shell.execute_reply.started":"2025-01-04T17:04:08.974996Z","shell.execute_reply":"2025-01-04T17:04:08.978379Z"}},"outputs":[],"execution_count":186},{"cell_type":"markdown","source":"Inspect Both Keys Again:\n\nCheck the refined_features keys and ensure they match the exact format of the ground_truth_captions keys.","metadata":{}},{"cell_type":"code","source":"# print(f\"Sample keys from refined_features: {list(refined_features.keys())[:5]}\")\n# print(f\"Sample keys from ground_truth_captions: {list(ground_truth_captions.keys())[:5]}\")\n\n# Inspecting refined features and ground truth captions for training and validation\nprint(\"\\nInspecting Keys from Refined Features and Ground Truth Captions:\")\n\n# Training dataset\nprint(\"\\nTraining Dataset:\")\nprint(f\"Sample Keys from Refined Features (Train): {list(refined_features_train.keys())[:5]}\")\nprint(f\"Sample Keys from Ground Truth Captions: {list(ground_truth_captions.keys())[:5]}\")\n\n# Validation dataset\nprint(\"\\nValidation Dataset:\")\nprint(f\"Sample Keys from Refined Features (Validation): {list(refined_features_val.keys())[:5]}\")\nprint(f\"Sample Keys from Ground Truth Captions: {list(ground_truth_captions.keys())[:5]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:35:49.405358Z","iopub.execute_input":"2025-01-04T17:35:49.405736Z","iopub.status.idle":"2025-01-04T17:35:49.411962Z","shell.execute_reply.started":"2025-01-04T17:35:49.405707Z","shell.execute_reply":"2025-01-04T17:35:49.411044Z"}},"outputs":[{"name":"stdout","text":"\nInspecting Keys from Refined Features and Ground Truth Captions:\n\nTraining Dataset:\nSample Keys from Refined Features (Train): ['LoeSv5enisE', 'v7U70gPylBA', 'XeXQ23fgS38', 'sb58mkT8_ak', 'A4pTxrrBtMU']\nSample Keys from Ground Truth Captions: ['GLd3aX16zBg', 'xHr8X2Wpmno', 'V53XmPeyjIU', 'mZwK0TBI1iY', 'H_9oM1Y60lU']\n\nValidation Dataset:\nSample Keys from Refined Features (Validation): ['arQZ2m0hMSU', 'OsYU9W14guU', 'bqmkNRNd5yw', 'OOX012L_cXA', 'HLQ16akzeI8']\nSample Keys from Ground Truth Captions: ['GLd3aX16zBg', 'xHr8X2Wpmno', 'V53XmPeyjIU', 'mZwK0TBI1iY', 'H_9oM1Y60lU']\n","output_type":"stream"}],"execution_count":201},{"cell_type":"markdown","source":"Normalize Keys:\n\nIf refined_features keys contain suffixes like _lo or any extra formatting, you can normalize them to match the keys in ground_truth_captions.","metadata":{}},{"cell_type":"code","source":"# normalized_refined_features = {\n#     key.split('_')[0]: value for key, value in refined_features.items()\n# }\n\n# print(f\"Normalized refined_features keys: {list(normalized_refined_features.keys())[:5]}\")\n\n\nnormalized_refined_features_train = {\n    key.split('_')[0]: value for key, value in refined_features_train.items()\n}\n\nprint(f\"Normalized refined_features keys: {list(normalized_refined_features_train.keys())[:5]}\")\n\n\nnormalized_refined_features_val = {\n    key.split('_')[0]: value for key, value in refined_features_val.items()\n}\n\nprint(f\"Normalized refined_features keys: {list(normalized_refined_features_val.keys())[:5]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:38:15.401490Z","iopub.execute_input":"2025-01-04T17:38:15.402297Z","iopub.status.idle":"2025-01-04T17:38:15.410115Z","shell.execute_reply.started":"2025-01-04T17:38:15.402250Z","shell.execute_reply":"2025-01-04T17:38:15.409073Z"}},"outputs":[{"name":"stdout","text":"Normalized refined_features keys: ['LoeSv5enisE', 'v7U70gPylBA', 'XeXQ23fgS38', 'sb58mkT8', 'A4pTxrrBtMU']\nNormalized refined_features keys: ['arQZ2m0hMSU', 'OsYU9W14guU', 'bqmkNRNd5yw', 'OOX012L', 'HLQ16akzeI8']\n","output_type":"stream"}],"execution_count":206},{"cell_type":"markdown","source":"Check Mapping Logic:\nIf there is a mapping between video_1, video_2, etc., and actual video IDs like 'oDsUh1es_lo', retrieve it and apply the mapping:","metadata":{}},{"cell_type":"code","source":"# aligned_keys = set(normalized_refined_features.keys()) & set(ground_truth_captions.keys())\n# print(f\"Number of aligned keys after normalization: {len(aligned_keys)}\")\n\naligned_keys_train = set(normalized_refined_features_train.keys()) & set(ground_truth_captions.keys())\nprint(f\"Number of aligned keys after normalization: {len(aligned_keys_train)}\")\n\naligned_keys_val = set(normalized_refined_features_val.keys()) & set(ground_truth_captions.keys())\nprint(f\"Number of aligned keys after normalization: {len(aligned_keys_val)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:40:14.091472Z","iopub.execute_input":"2025-01-04T17:40:14.091869Z","iopub.status.idle":"2025-01-04T17:40:14.098266Z","shell.execute_reply.started":"2025-01-04T17:40:14.091838Z","shell.execute_reply":"2025-01-04T17:40:14.097362Z"}},"outputs":[{"name":"stdout","text":"Number of aligned keys after normalization: 562\nNumber of aligned keys after normalization: 147\n","output_type":"stream"}],"execution_count":208},{"cell_type":"markdown","source":"Re-filter refined_features:\nAfter ensuring the keys align:\n\npython\nCopy code\n","metadata":{}},{"cell_type":"code","source":"# filtered_refined_features = {\n#     video_id: normalized_refined_features[video_id]\n#     for video_id in aligned_keys\n# }\n\n# print(f\"Filtered refined features: {len(filtered_refined_features)} videos\")\nfiltered_refined_features_train = {\n    video_id: normalized_refined_features_train[video_id]\n    for video_id in aligned_keys_train\n}\n\nprint(f\"Filtered refined features: {len(filtered_refined_features_train)} videos\")\n\n\nfiltered_refined_features_val = {\n    video_id: normalized_refined_features_val[video_id]\n    for video_id in aligned_keys_val\n}\n\nprint(f\"Filtered refined features: {len(filtered_refined_features_val)} videos\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:42:19.675647Z","iopub.execute_input":"2025-01-04T17:42:19.676396Z","iopub.status.idle":"2025-01-04T17:42:19.681775Z","shell.execute_reply.started":"2025-01-04T17:42:19.676366Z","shell.execute_reply":"2025-01-04T17:42:19.680853Z"}},"outputs":[{"name":"stdout","text":"Filtered refined features: 562 videos\nFiltered refined features: 147 videos\n","output_type":"stream"}],"execution_count":211},{"cell_type":"markdown","source":"complete snippet to handle ground truth and missmatch:","metadata":{}},{"cell_type":"code","source":"# # Normalize refined_features keys\n# normalized_refined_features = {\n#     key.split('_')[0]: value for key, value in refined_features.items()\n# }\n\n# # Check alignment\n# aligned_keys = set(normalized_refined_features.keys()) & set(ground_truth_captions.keys())\n# print(f\"Number of aligned keys after normalization: {len(aligned_keys)}\")\n\n# # Filter refined features\n# filtered_refined_features = {\n#     video_id: normalized_refined_features[video_id]\n#     for video_id in aligned_keys\n# }\n\n# print(f\"Filtered refined features: {len(filtered_refined_features)} videos\")\n\n# # Example: Check a filtered video ID\n# sample_video_id = list(filtered_refined_features.keys())[0]\n# print(f\"Refined features for video {sample_video_id}: {filtered_refined_features[sample_video_id].shape}\")\n\n\n# Normalize, align, and filter refined features for training and validation datasets\nprint(\"\\nProcessing Refined Features and Alignment with Ground Truth Captions:\")\n\n# Training dataset\nprint(\"\\nTraining Dataset:\")\nnormalized_refined_features_train = {\n    key.split('_')[0]: value for key, value in refined_features_train.items()\n}\naligned_keys_train = set(normalized_refined_features_train.keys()) & set(ground_truth_captions.keys())\nprint(f\"Number of aligned keys in Training Data after normalization: {len(aligned_keys_train)}\")\n\nfiltered_refined_features_train = {\n    video_id: normalized_refined_features_train[video_id]\n    for video_id in aligned_keys_train\n}\nprint(f\"Filtered refined features for Training Data: {len(filtered_refined_features_train)} videos\")\n\n# Example: Check a filtered video ID from training dataset\nif filtered_refined_features_train:\n    sample_video_id_train = list(filtered_refined_features_train.keys())[0]\n    print(f\"Refined features for video {sample_video_id_train} (Train): {filtered_refined_features_train[sample_video_id_train].shape}\")\n\n# Validation dataset\nprint(\"\\nValidation Dataset:\")\nnormalized_refined_features_val = {\n    key.split('_')[0]: value for key, value in refined_features_val.items()\n}\naligned_keys_val = set(normalized_refined_features_val.keys()) & set(ground_truth_captions.keys())\nprint(f\"Number of aligned keys in Validation Data after normalization: {len(aligned_keys_val)}\")\n\nfiltered_refined_features_val = {\n    video_id: normalized_refined_features_val[video_id]\n    for video_id in aligned_keys_val\n}\nprint(f\"Filtered refined features for Validation Data: {len(filtered_refined_features_val)} videos\")\n\n# Example: Check a filtered video ID from validation dataset\nif filtered_refined_features_val:\n    sample_video_id_val = list(filtered_refined_features_val.keys())[0]\n    print(f\"Refined features for video {sample_video_id_val} (Validation): {filtered_refined_features_val[sample_video_id_val].shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:43:37.405109Z","iopub.execute_input":"2025-01-04T17:43:37.405475Z","iopub.status.idle":"2025-01-04T17:43:37.415221Z","shell.execute_reply.started":"2025-01-04T17:43:37.405433Z","shell.execute_reply":"2025-01-04T17:43:37.414303Z"}},"outputs":[{"name":"stdout","text":"\nProcessing Refined Features and Alignment with Ground Truth Captions:\n\nTraining Dataset:\nNumber of aligned keys in Training Data after normalization: 562\nFiltered refined features for Training Data: 562 videos\nRefined features for video Epl4sPuetO0 (Train): (15, 10)\n\nValidation Dataset:\nNumber of aligned keys in Validation Data after normalization: 147\nFiltered refined features for Validation Data: 147 videos\nRefined features for video G-vY8T7pSTM (Validation): (15, 10)\n","output_type":"stream"}],"execution_count":214},{"cell_type":"code","source":"# # Normalize and align video IDs between features and captions\n# normalized_refined_features = {\n#     key.split('_')[0]: value for key, value in refined_features.items()\n# }\n# aligned_keys = set(normalized_refined_features.keys()) & set(ground_truth_captions.keys())\n\n# print(f\"Number of aligned keys: {len(aligned_keys)}\")\n\n# # Filter both refined features and captions based on aligned keys\n# filtered_refined_features = {\n#     video_id: normalized_refined_features[video_id] for video_id in aligned_keys\n# }\n# filtered_ground_truth_captions = {\n#     video_id: ground_truth_captions[video_id] for video_id in aligned_keys\n# }\n\n# print(f\"Filtered refined features: {len(filtered_refined_features)}\")\n# print(f\"Filtered ground truth captions: {len(filtered_ground_truth_captions)}\")\n\n\n# Normalize, align, and filter refined features and ground truth captions for training and validation datasets\nprint(\"\\nProcessing Refined Features and Ground Truth Captions:\")\n\n\n# Training dataset\nprint(\"\\nTraining Dataset:\")\nnormalized_refined_features_train = {\n    key.split('_')[0]: value for key, value in refined_features_train.items()\n}\n\n\naligned_keys_train = set(normalized_refined_features_train.keys()) & set(ground_truth_captions.keys())\nprint(f\"Number of aligned keys in Training Data: {len(aligned_keys_train)}\")\n\n\n\nfiltered_refined_features_train = {\n    video_id: normalized_refined_features_train[video_id] for video_id in aligned_keys_train\n}\n\n\nfiltered_ground_truth_captions_train = {\n    video_id: ground_truth_captions[video_id] for video_id in aligned_keys_train\n}\n\n\nprint(f\"Filtered refined features for Training Data: {len(filtered_refined_features_train)}\")\nprint(f\"Filtered ground truth captions for Training Data: {len(filtered_ground_truth_captions_train)}\")\n\n\n\n# Validation dataset\nprint(\"\\nValidation Dataset:\")\nnormalized_refined_features_val = {\n    key.split('_')[0]: value for key, value in refined_features_val.items()\n}\naligned_keys_val = set(normalized_refined_features_val.keys()) & set(ground_truth_captions.keys())\n\nprint(f\"Number of aligned keys in Validation Data: {len(aligned_keys_val)}\")\n\nfiltered_refined_features_val = {\n    video_id: normalized_refined_features_val[video_id] for video_id in aligned_keys_val\n}\nfiltered_ground_truth_captions_val = {\n    video_id: ground_truth_captions[video_id] for video_id in aligned_keys_val\n}\n\nprint(f\"Filtered refined features for Validation Data: {len(filtered_refined_features_val)}\")\nprint(f\"Filtered ground truth captions for Validation Data: {len(filtered_ground_truth_captions_val)}\")\n\n\n\n\n# Example: Print a sample filtered video ID from training dataset\nif filtered_refined_features_train:\n    sample_video_id_train = list(filtered_refined_features_train.keys())[0]\n    print(f\"\\nSample Training Video ID: {sample_video_id_train}\")\n    print(f\"Training Refined Features Shape: {filtered_refined_features_train[sample_video_id_train].shape}\")\n    print(f\"Training Captions: {filtered_ground_truth_captions_train[sample_video_id_train]}\")\n\n# Example: Print a sample filtered video ID from validation dataset\nif filtered_refined_features_val:\n    sample_video_id_val = list(filtered_refined_features_val.keys())[0]\n    print(f\"\\nSample Validation Video ID: {sample_video_id_val}\")\n    print(f\"Validation Refined Features Shape: {filtered_refined_features_val[sample_video_id_val].shape}\")\n    print(f\"Validation Captions: {filtered_ground_truth_captions_val[sample_video_id_val]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:52:37.676756Z","iopub.execute_input":"2025-01-04T17:52:37.677603Z","iopub.status.idle":"2025-01-04T17:52:37.688819Z","shell.execute_reply.started":"2025-01-04T17:52:37.677566Z","shell.execute_reply":"2025-01-04T17:52:37.687938Z"}},"outputs":[{"name":"stdout","text":"\nProcessing Refined Features and Ground Truth Captions:\n\nTraining Dataset:\nNumber of aligned keys in Training Data: 562\nFiltered refined features for Training Data: 562\nFiltered ground truth captions for Training Data: 562\n\nValidation Dataset:\nNumber of aligned keys in Validation Data: 147\nFiltered refined features for Validation Data: 147\nFiltered ground truth captions for Validation Data: 147\n\nSample Training Video ID: Epl4sPuetO0\nTraining Refined Features Shape: (15, 10)\nTraining Captions: ['add meat and spice packet to a pan', 'stir the meat', 'add coconut milk', 'cook noodles in a pot', 'place cooked noodles in a bowl and pour liquid from meat on top', 'top with meat and bean sprouts']\n\nSample Validation Video ID: G-vY8T7pSTM\nValidation Refined Features Shape: (15, 10)\nValidation Captions: ['press rice down on half a sheet of nori', 'add toasted sesame seeds or masago', 'arrange crab in a line', 'add cucumber and avocado', 'roll up and press down', 'cut the roll into eight pieces']\n","output_type":"stream"}],"execution_count":215},{"cell_type":"markdown","source":"# Implementing caption Generation","metadata":{}},{"cell_type":"markdown","source":"Dataset Preparation:\nCaptionDataset aligns refined video features with corresponding captions.\nCaptions are tokenized, padded, and truncated to a fixed sequence length.\n\nModel Architecture:\nThe model encodes temporally refined features using a linear layer and positional encodings.\nCaptions are embedded and passed to the Transformer decoder, which predicts the next token for each time step.\n\nTraining Loop:\nOptimizes the cross-entropy loss between predicted tokens and ground-truth captions.\nUses teacher forcing by providing ground-truth tokens as input during training.\n\nInference:\nStarts with a [CLS] token and iteratively predicts the next token until a [SEP] token is generated or the maximum sequence length is reached.","metadata":{}},{"cell_type":"markdown","source":"## Model Implementation\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nfrom transformers import AutoTokenizer\n\n# Define the Transformer-based Caption Generation Model\nclass CaptionGenerationModel(nn.Module):\n    def __init__(self, input_dim, vocab_size, embed_dim, num_heads, num_layers, max_seq_length):\n        super(CaptionGenerationModel, self).__init__()\n        self.encoder = nn.Linear(input_dim, embed_dim)\n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_encoding = nn.Embedding(max_seq_length, embed_dim)  # Match max_seq_length\n        self.decoder = nn.Transformer(\n            d_model=embed_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n        )\n        self.fc_out = nn.Linear(embed_dim, vocab_size)\n\n    # def forward(self, features, captions):\n    #     # Encode input features\n    #     encoded_features = self.encoder(features)\n    #     encoded_features += self.positional_encoding.weight[:encoded_features.size(1), :]\n    \n    #     # Embed captions\n    #     caption_embeddings = self.token_embedding(captions)\n    #     seq_length = caption_embeddings.size(1)  # Current sequence length\n    #     caption_embeddings += self.positional_encoding.weight[:seq_length, :]  # Adjust positional encoding\n    \n    #     # Decode captions\n    #     output = self.decoder(\n    #         src=encoded_features,  # [Batch, Seq_len, Embed_dim]\n    #         tgt=caption_embeddings,  # [Batch, Seq_len, Embed_dim]\n    #     )\n    #     return self.fc(output)\n    def forward(self, features, captions):\n        # Ensure features have the same sequence length as captions\n        max_seq_length = captions.size(1)  # The target sequence length from captions\n        seq_len = features.size(1)\n    \n        # If features are shorter, pad them\n        if seq_len < max_seq_length:\n            padding = torch.zeros(\n                (features.size(0), max_seq_length - seq_len, features.size(2)),  # (batch_size, pad_len, feature_dim)\n                device=features.device\n            )\n            features = torch.cat([features, padding], dim=1)\n    \n        # If features are longer, truncate them\n        elif seq_len > max_seq_length:\n            features = features[:, :max_seq_length, :]\n    \n        # Encode input features\n        encoded_features = self.encoder(features)\n        encoded_features += self.positional_encoding.weight[:encoded_features.size(1), :]\n    \n        # Embed captions\n        caption_embeddings = self.token_embedding(captions)\n        seq_length = caption_embeddings.size(1)  # Current sequence length\n        caption_embeddings += self.positional_encoding.weight[:seq_length, :]  # Adjust positional encoding\n    \n        # Decode captions\n        output = self.decoder(\n            src=encoded_features,  # [Batch, Seq_len, Embed_dim]\n            tgt=caption_embeddings,  # [Batch, Seq_len, Embed_dim]\n        )\n        return self.fc_out(output)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset Implementation","metadata":{}},{"cell_type":"code","source":"class CaptionDataset(torch.utils.data.Dataset):\n    def __init__(self, features, captions, tokenizer, max_seq_length):\n        self.features = features  # Refined features (dict with video IDs as keys)\n        self.captions = captions  # Ground truth captions (dict with video IDs as keys)\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.video_ids = list(features.keys())\n\n    def __len__(self):\n        return len(self.video_ids)\n\n    def __getitem__(self, idx):\n        video_id = self.video_ids[idx]\n        features = torch.tensor(self.features[video_id], dtype=torch.float32)\n\n        # Get the ground truth caption for the video ID\n        caption = self.captions[video_id][0]  # Assuming captions are stored as lists, take the first caption\n\n        # Tokenize the caption\n        tokenized_caption = self.tokenizer(\n            caption,\n            max_length=self.max_seq_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )[\"input_ids\"].squeeze(0)  # Squeeze to remove batch dimension\n\n        return features, tokenized_caption\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define a Custom Collate Function\n\nThe custom collate function will:\n\nDynamically pad captions to the maximum length in the batch.\nStack the features and padded captions into tensors.","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    features = []\n    captions = []\n\n    for feature, caption in batch:\n        features.append(feature)\n        captions.append(caption)\n\n    # Stack features (no padding needed as they are fixed-size)\n    features = torch.stack(features)\n\n    # Pad captions to the maximum length in this batch\n    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n\n    return features, captions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setup and Hyperparameters","metadata":{}},{"cell_type":"code","source":"# # Initialize tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your model's tokenizer\n\n# # # Example ground truth captions (replace with actual data)\n# # ground_truth_captions = {\n# #     \"video_1\": \"A person is cooking pasta.\",\n# #     \"video_2\": \"The chef chops onions and garlic.\",\n# # }\n\n# # Hyperparameters\n# input_dim = refined_features[list(refined_features.keys())[0]].shape[1]  # Dimension of refined features\n# embed_dim = 512\n# vocab_size = tokenizer.vocab_size\n# num_heads = 8\n# num_layers = 4\n# max_seq_length = 50  # Maximum caption length\n\n# # Prepare dataset and dataloader\n# # caption_dataset = CaptionDataset(refined_features, ground_truth_captions, tokenizer, max_seq_length)\n# # caption_dataloader = torch.utils.data.DataLoader(caption_dataset, batch_size=16, shuffle=True)\n# # Initialize the dataset with filtered features and captions\n# caption_dataset = CaptionDataset(\n#     features=filtered_refined_features,\n#     captions=filtered_ground_truth_captions,\n#     tokenizer=tokenizer,\n#     max_seq_length=max_seq_length\n# )\n# caption_dataloader = torch.utils.data.DataLoader(\n#     caption_dataset,\n#     batch_size=16,\n#     shuffle=True,\n#     collate_fn=collate_fn  # Use custom collate function\n# )\n\n\n# Initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your model's tokenizer\n\n# Hyperparameters\ninput_dim = refined_features_train[list(refined_features_train.keys())[0]].shape[1]  # Dimension of refined features\nembed_dim = 512\nvocab_size = tokenizer.vocab_size\nnum_heads = 8\nnum_layers = 4\nmax_seq_length = 50  # Maximum caption length\n\n# Prepare datasets and dataloaders for training and validation\nprint(\"\\nPreparing Training Dataset and Dataloader:\")\ncaption_dataset_train = CaptionDataset(\n    features=filtered_refined_features_train,\n    captions=filtered_ground_truth_captions_train,\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length\n)\ncaption_dataloader_train = torch.utils.data.DataLoader(\n    caption_dataset_train,\n    batch_size=16,\n    shuffle=True,\n    collate_fn=collate_fn  # Use custom collate function\n)\nprint(f\"Training Dataset Size: {len(caption_dataset_train)}\")\n\nprint(\"\\nPreparing Validation Dataset and Dataloader:\")\ncaption_dataset_val = CaptionDataset(\n    features=filtered_refined_features_val,\n    captions=filtered_ground_truth_captions_val,\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length\n)\ncaption_dataloader_val = torch.utils.data.DataLoader(\n    caption_dataset_val,\n    batch_size=16,\n    shuffle=False,\n    collate_fn=collate_fn  # Use custom collate function\n)\nprint(f\"Validation Dataset Size: {len(caption_dataset_val)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Verify the Shapes in the DataLoader","metadata":{}},{"cell_type":"code","source":"# for features, captions in caption_dataloader:\n#     print(f\"Feature shape: {features.shape}\")  # Should be (batch_size, seq_len, input_dim)\n#     print(f\"Caption shape: {captions.shape}\")  # Should be (batch_size, max_seq_length)\n#     break\n\nprint(\"for train...\")\nfor features, captions in caption_dataloader_train:\n    print(f\"Feature shape: {features.shape}\")  # Should be (batch_size, seq_len, input_dim)\n    print(f\"Caption shape: {captions.shape}\")  # Should be (batch_size, max_seq_length)\n    break\n\nprint(\"for val...\")\nfor features, captions in caption_dataloader_val:\n    print(f\"Feature shape: {features.shape}\")  # Should be (batch_size, seq_len, input_dim)\n    print(f\"Caption shape: {captions.shape}\")  # Should be (batch_size, max_seq_length)\n    break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialize model, loss function, and optimizer\nand setup gpu and also Pass the custom collate_fn to the DataLoader:","metadata":{}},{"cell_type":"code","source":"# # Move model to GPU\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# caption_model.to(device)\n\n# Initialize model, loss function, and optimizer\ncaption_model = CaptionGenerationModel(input_dim, vocab_size, embed_dim, num_heads, num_layers, max_seq_length)\ncaption_model = caption_model.to(device)  # Move model to the device\ncriterion = nn.CrossEntropyLoss(ignore_index=0).to(device)  # Move loss function to the device\noptimizer = torch.optim.Adam(caption_model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"# caption_model.train()\n# num_epochs = 50\n\n# for epoch in range(num_epochs):\n#     total_loss = 0.0\n#     for features, captions in caption_dataloader:\n#         features = features.to(device)  # Move features to device\n#         captions = captions.to(device)  # Move captions to device\n\n#         optimizer.zero_grad()\n#         outputs = caption_model(features, captions[:, :-1])  # Predict next tokens\n#         loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))  # Ignore start token\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n\n#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n\n\ncaption_model.train()\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    # Training phase\n    total_train_loss = 0.0\n    caption_model.train()  # Set the model to training mode\n    for features, captions in caption_dataloader_train:\n        features = features.to(device)  # Move features to device\n        captions = captions.to(device)  # Move captions to device\n\n        optimizer.zero_grad()\n        outputs = caption_model(features, captions[:, :-1])  # Predict next tokens\n        loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))  # Ignore start token\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(caption_dataloader_train)\n\n    # Validation phase\n    total_val_loss = 0.0\n    caption_model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for validation\n        for features, captions in caption_dataloader_val:\n            features = features.to(device)  # Move features to device\n            captions = captions.to(device)  # Move captions to device\n\n            outputs = caption_model(features, captions[:, :-1])  # Predict next tokens\n            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))  # Ignore start token\n            total_val_loss += loss.item()\n\n    avg_val_loss = total_val_loss / (len(caption_dataloader_val)+1) #extra 1\n\n    # Log epoch results\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference:\n\nStarts with a [CLS] token and iteratively predicts the next token until a [SEP] token is generated or the maximum sequence length is reached.","metadata":{}},{"cell_type":"code","source":"# caption_model.eval()\n# generated_captions = {}\n\n# with torch.no_grad():\n#     for video_id, features in refined_features.items():\n#         features = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n#         generated_tokens = torch.zeros(1, max_seq_length).long().to(\"cuda\")  # Start with [START] token\n#         for t in range(1, max_seq_length):\n#             outputs = caption_model(features, generated_tokens[:, :t])\n#             next_token = outputs[:, t - 1].argmax(dim=-1)\n#             generated_tokens[:, t] = next_token\n#             if next_token.item() == tokenizer.eos_token_id:  # Stop if [END] token is predicted\n#                 break\n#         generated_captions[video_id] = tokenizer.decode(generated_tokens[0].tolist(), skip_special_tokens=True)\n\n# # Print generated captions\n# for video_id, caption in list(generated_captions.items())[:5]:  # Print first 5 captions\n#     print(f\"Video ID: {video_id}, Caption: {caption}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from nltk.translate.bleu_score import sentence_bleu\n\n# # Example evaluation\n# for video_id in ground_truth_captions.keys():\n#     reference = [ground_truth_captions[video_id].split()]\n#     hypothesis = generated_captions[video_id].split()\n#     score = sentence_bleu(reference, hypothesis)\n#     print(f\"Video ID: {video_id}, BLEU Score: {score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\ncaption_counts = Counter(caption for captions in ground_truth_captions.values() for caption in captions)\nprint(caption_counts.most_common(10))  # Inspect the most frequent captions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption_for_proposal(model, proposal_feature, tokenizer, max_seq_length, temperature=1.0, top_k=5):\n    model.eval()  # Set model to evaluation mode\n    \n    # Convert proposal_feature to PyTorch tensor\n    proposal_feature = torch.tensor(proposal_feature, dtype=torch.float32)\n    \n    # Ensure proposal_feature has three dimensions (batch_size, seq_length, feature_dim)\n    if len(proposal_feature.shape) == 1:  # If shape is (feature_dim,)\n        proposal_feature = proposal_feature.unsqueeze(0).unsqueeze(0)  # Add seq_length and batch dimensions\n    elif len(proposal_feature.shape) == 2:  # If shape is (seq_length, feature_dim)\n        proposal_feature = proposal_feature.unsqueeze(0)  # Add batch dimension\n    \n    # Move to device\n    proposal_feature = proposal_feature.to(device)\n\n    generated_caption = []\n    input_ids = torch.tensor([tokenizer.cls_token_id], dtype=torch.long).unsqueeze(0).to(device)  # Start with CLS token\n    \n    for _ in range(max_seq_length):\n        with torch.no_grad():\n            logits = model(proposal_feature, input_ids)  # Forward pass through the decoder\n            logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n            probabilities = torch.softmax(logits, dim=-1)\n            \n            # Sampling from Top-K probabilities\n            top_k_probs, top_k_indices = torch.topk(probabilities, k=top_k, dim=-1)\n            next_token_index = top_k_probs.squeeze().multinomial(1).item()  # Sample from probabilities\n            next_token = top_k_indices.squeeze()[next_token_index]  # Map index back to token ID\n            \n            generated_caption.append(next_token.item())\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n\n            if next_token.item() == tokenizer.sep_token_id:  # Stop if SEP token is generated\n                break\n\n    # Decode generated tokens to text\n    caption = tokenizer.decode(generated_caption, skip_special_tokens=True)\n    return caption\n\n\nprint(\"Generating captions for filtered proposals...\")\nfor video_id, proposal_list in filtered_proposals_train.items():\n    if video_id not in refined_features_train:\n        print(f\"  Warning: Video ID {video_id} not found in refined_features_train. Skipping...\")\n        continue\n\n    video_features = refined_features_train[video_id]\n    num_frames = video_features.shape[0]\n    print(f\"Video ID: {video_id}, Feature shape: {video_features.shape}\")\n\n    for idx, proposal in enumerate(proposal_list):\n        # Extract start and end times\n        start_time = proposal.get(\"start\", 0.0)\n        end_time = proposal.get(\"end\", 1.0)\n        print(f\"  Proposal {idx + 1}: Start time={start_time}, End time={end_time}\")\n\n        # Scale to match frame indices\n        start_idx = int(start_time * num_frames)\n        end_idx = int(end_time * num_frames)\n\n        # Check if indices are valid\n        if start_idx < 0 or end_idx > num_frames or start_idx >= end_idx:\n            print(f\"  Proposal {idx + 1} has invalid indices: Start={start_idx}, End={end_idx}. Skipping...\")\n            continue\n\n        # Extract proposal feature\n        proposal_feature = video_features[start_idx:end_idx]\n        if proposal_feature.shape[0] == 0:  # Skip empty proposals\n            print(f\"  Proposal {idx + 1} has no valid frames. Skipping...\")\n            continue\n\n        # Apply mean pooling\n        proposal_feature = np.mean(proposal_feature, axis=0)\n        # print(f\"  Proposal {idx + 1}: Feature shape after pooling={proposal_feature.shape}\")\n\n        # Generate caption\n        caption = generate_caption_for_proposal(caption_model, proposal_feature, tokenizer, max_seq_length)\n        # print(f\"  Proposal {idx + 1}: Start={start_idx}, End={end_idx}, Caption={caption}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption_for_proposal(model, proposal_feature, tokenizer, max_seq_length, temperature=1.0, top_k=5):\n    model.eval()  # Set model to evaluation mode\n    \n    # Convert proposal_feature to PyTorch tensor\n    proposal_feature = torch.tensor(proposal_feature, dtype=torch.float32)\n    \n    # Ensure proposal_feature has three dimensions (batch_size, seq_length, feature_dim)\n    if len(proposal_feature.shape) == 1:  # If shape is (feature_dim,)\n        proposal_feature = proposal_feature.unsqueeze(0).unsqueeze(0)  # Add seq_length and batch dimensions\n    elif len(proposal_feature.shape) == 2:  # If shape is (seq_length, feature_dim)\n        proposal_feature = proposal_feature.unsqueeze(0)  # Add batch dimension\n    \n    # Move to device\n    proposal_feature = proposal_feature.to(device)\n\n    generated_caption = []\n    input_ids = torch.tensor([tokenizer.cls_token_id], dtype=torch.long).unsqueeze(0).to(device)  # Start with CLS token\n    \n    for step in range(max_seq_length):\n        with torch.no_grad():\n            logits = model(proposal_feature, input_ids)  # Forward pass through the decoder\n            # print(f\"Step {step + 1}: Logits shape: {logits.shape}\")  # Debug logits\n            \n            logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n            probabilities = torch.softmax(logits, dim=-1)\n            \n            # Sampling from Top-K probabilities\n            top_k_probs, top_k_indices = torch.topk(probabilities, k=top_k, dim=-1)\n            # print(f\"Top-K Tokens: {top_k_indices.squeeze()}, Top-K Probabilities: {top_k_probs.squeeze()}\")  # Debug\n            \n            next_token_index = top_k_probs.squeeze().multinomial(1).item()  # Sample from probabilities\n            next_token = top_k_indices.squeeze()[next_token_index]  # Map index back to token ID\n            \n            # print(f\"Generated Token: {next_token}\")  # Debug generated token\n            \n            generated_caption.append(next_token.item())\n            input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)\n\n            if next_token.item() == tokenizer.sep_token_id:  # Stop if SEP token is generated\n                break\n\n    # Decode generated tokens to text\n    caption = tokenizer.decode(generated_caption, skip_special_tokens=True)\n    # print(f\"Generated Caption: {caption}\")  # Debug final caption\n    return caption\n\n\n# Initialize a dictionary to store video-wise captions\nvideo_captions = {}\n\nprint(\"Generating captions for filtered proposals...\")\nfor video_id, proposal_list in filtered_proposals_train.items():\n    if video_id not in refined_features_train:\n        # print(f\"  Warning: Video ID {video_id} not found in refined_features_train. Skipping...\")\n        continue\n\n    video_features = refined_features_train[video_id]\n    num_frames = video_features.shape[0]\n    # print(f\"Video ID: {video_id}, Feature shape: {video_features.shape}\")\n\n        # Initialize a list to store captions for all proposals of this video\n    merged_caption = []\n\n    for idx, proposal in enumerate(proposal_list):\n        # Extract start and end times\n        start_time = proposal.get(\"start\", 0.0)\n        end_time = proposal.get(\"end\", 1.0)\n        # print(f\"  Proposal {idx + 1}: Start time={start_time}, End time={end_time}\")\n\n\n\n        # Scale to match frame indices\n        start_idx = int(start_time * num_frames)\n        end_idx = int(end_time * num_frames)\n\n        # Check if indices are valid\n        if start_idx < 0 or end_idx > num_frames or start_idx >= end_idx:\n            # print(f\"  Proposal {idx + 1} has invalid indices: Start={start_idx}, End={end_idx}. Skipping...\")\n            continue\n\n        # Extract proposal feature\n        proposal_feature = video_features[start_idx:end_idx]\n        if proposal_feature.shape[0] == 0:  # Skip empty proposals\n            # print(f\"  Proposal {idx + 1} has no valid frames. Skipping...\")\n            continue\n\n        # Apply mean pooling\n        proposal_feature = np.mean(proposal_feature, axis=0)\n        # print(f\"  Proposal {idx + 1}: Feature shape after pooling={proposal_feature.shape}\")\n\n        # Generate caption\n        caption = generate_caption_for_proposal(caption_model, proposal_feature, tokenizer, max_seq_length)\n        # print(f\"  Proposal {idx + 1}: Start={start_idx}, End={end_idx}, Caption={caption}\")\n\n        # Append the proposal caption to the merged_caption list\n        merged_caption.append(caption)\n\n    # Combine all proposal captions into a single caption for the video\n    video_captions[video_id] = \" \".join(merged_caption)\n    # print(f\"Generated Video-wise Caption for {video_id}: {video_captions[video_id]}\")\n\n# Print all video-wise captions\nprint(\"\\nVideo-wise Captions:\")\nfor video_id, caption in video_captions.items():\n    print(f\"Video ID: {video_id}, Caption: {caption}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"OPTIMIZING THE ABove code for code generation","metadata":{}},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"markdown","source":"Install the required Python libraries for evaluation metrics.","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu rouge_score pycocoevalcap\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Below is evaluation code 1 ","metadata":{}},{"cell_type":"code","source":"# !pip install rouge --quiet\n\n# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# from rouge import Rouge\n\n# # Function to evaluate a single caption\n# def evaluate_caption(generated_caption, ground_truth_captions):\n#     # BLEU Score\n#     smoothie = SmoothingFunction().method4\n#     bleu_score = sentence_bleu(\n#         [caption.split() for caption in ground_truth_captions],\n#         generated_caption.split(),\n#         smoothing_function=smoothie\n#     )\n    \n#     # ROUGE Score\n#     rouge = Rouge()\n#     rouge_score = rouge.get_scores([generated_caption], ground_truth_captions, avg=True)\n    \n#     return bleu_score, rouge_score\n\n# # Generate captions for test data\n# generated_captions = {}\n# ground_truth_captions = {}\n\n# for video_id, features in test_refined_features.items():\n#     # Generate caption for each video\n#     generated_caption = generate_caption(caption_model, features, tokenizer, max_seq_length)\n#     generated_captions[video_id] = generated_caption\n    \n#     # Ground truth captions\n#     ground_truth_captions[video_id] = test_ground_truth_captions[video_id]\n\n# # Evaluate all captions\n# bleu_scores = []\n# rouge_scores = []\n\n# for video_id in generated_captions:\n#     generated = generated_captions[video_id]\n#     ground_truth = ground_truth_captions[video_id]\n#     bleu, rouge = evaluate_caption(generated, ground_truth)\n#     bleu_scores.append(bleu)\n#     rouge_scores.append(rouge)\n\n# # Print average scores\n# print(f\"Average BLEU Score: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n# print(f\"Average ROUGE Score: {sum([r['rouge-l']['f'] for r in rouge_scores]) / len(rouge_scores):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation code 2","metadata":{}},{"cell_type":"markdown","source":"Define Metric Functions\n1. Caption Quality Metrics (BLEU, METEOR, CIDEr, ROUGE-L)","metadata":{}},{"cell_type":"code","source":"from pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.rouge.rouge import Rouge\n\ndef evaluate_caption_metrics(references, hypotheses):\n    \"\"\"\n    Compute BLEU, METEOR, CIDEr, and ROUGE-L metrics.\n    :param references: Dictionary of {video_id: list of ground-truth captions}\n    :param hypotheses: Dictionary of {video_id: generated caption}\n    :return: Dictionary of metric scores.\n    \"\"\"\n    scorers = {\n        \"BLEU\": Bleu(4),  # Compute BLEU-1, BLEU-2, BLEU-3, and BLEU-4\n        \"METEOR\": Meteor(),\n        \"CIDEr\": Cider(),\n        \"ROUGE\": Rouge()\n    }\n    \n    scores = {}\n    for metric, scorer in scorers.items():\n        score, _ = scorer.compute_score(references, hypotheses)\n        if metric == \"BLEU\":\n            scores.update({f\"BLEU-{i + 1}\": score[i] for i in range(4)})\n        else:\n            scores[metric] = score\n    \n    return scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Proposal Quality Metrics (Recall@K and Precision@K)\nFor proposal evaluation, calculate Recall@K and Precision@K for multiple temporal Intersection over Union (tIoU) thresholds.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_recall_precision(proposals, ground_truths, iou_thresholds, k_values):\n    \"\"\"\n    Compute Recall@K and Precision@K for proposal quality evaluation.\n    :param proposals: List of proposals [video_id, start, end, confidence]\n    :param ground_truths: List of ground-truth intervals [video_id, start, end]\n    :param iou_thresholds: List of IoU thresholds (e.g., [0.3, 0.5, 0.7, 0.9])\n    :param k_values: List of K values for evaluation (e.g., [1, 5, 10])\n    :return: Dictionary of Recall@K and Precision@K scores.\n    \"\"\"\n    def compute_iou(interval1, interval2):\n        intersection = max(0, min(interval1[1], interval2[1]) - max(interval1[0], interval2[0]))\n        union = max(interval1[1], interval2[1]) - min(interval1[0], interval2[0])\n        return intersection / union\n\n    results = {}\n    for iou_threshold in iou_thresholds:\n        for k in k_values:\n            recall_scores = []\n            precision_scores = []\n            for video_id in set([p[0] for p in proposals]):\n                # Filter proposals and ground truths for the current video\n                video_proposals = [p[1:] for p in proposals if p[0] == video_id]\n                video_ground_truths = [g[1:] for g in ground_truths if g[0] == video_id]\n                \n                # Sort proposals by confidence and select top-K\n                video_proposals = sorted(video_proposals, key=lambda x: x[2], reverse=True)[:k]\n                \n                # Compute IoU for each proposal against all ground truths\n                matches = 0\n                for gt in video_ground_truths:\n                    for prop in video_proposals:\n                        if compute_iou(prop[:2], gt) >= iou_threshold:\n                            matches += 1\n                            break\n                \n                recall = matches / len(video_ground_truths) if video_ground_truths else 0\n                precision = matches / len(video_proposals) if video_proposals else 0\n                \n                recall_scores.append(recall)\n                precision_scores.append(precision)\n            \n            results[f\"Recall@{k}_tIoU@{iou_threshold}\"] = np.mean(recall_scores)\n            results[f\"Precision@{k}_tIoU@{iou_threshold}\"] = np.mean(precision_scores)\n    \n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluate the Model\n1. Caption Quality Evaluation\nRun the caption evaluation on a test set.","metadata":{}},{"cell_type":"code","source":"# # Define references and hypotheses\n# references = {video_id: [caption for caption in captions] for video_id, captions in filtered_ground_truth_captions_train.items()}\n# hypotheses = {}\n\n# for video_id, feature in filtered_refined_features_train.items():\n#     generated_caption = generate_caption_for_proposal(caption_model, feature, tokenizer, max_seq_length)\n#     hypotheses[video_id] = [generated_caption]  # Wrap the generated caption in a list\n\n# # Compute metrics\n# caption_metrics = evaluate_caption_metrics(references, hypotheses)\n# print(\"Caption Quality Metrics:\")\n# for metric, score in caption_metrics.items():\n#     print(f\"{metric}: {score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define references and hypotheses\nreferences = {video_id: [caption for caption in captions]\n              for video_id, captions in filtered_ground_truth_captions_train.items()}\n\n# Generate hypotheses (captions) for all video IDs\nhypotheses = {}\nfor video_id, feature in filtered_refined_features_train.items():\n    generated_caption = generate_caption_for_proposal(caption_model, feature, tokenizer, max_seq_length)\n    hypotheses[video_id] = [generated_caption]  # Wrap the generated caption in a list\n\n# Split video IDs into train and test sets\nvideo_ids = list(references.keys())\ntrain_ids, test_ids = train_test_split(video_ids, test_size=0.9, random_state=42)\n\n# Split references and hypotheses into train and test\nreferences_train = {video_id: references[video_id] for video_id in train_ids}\nhypotheses_train = {video_id: hypotheses[video_id] for video_id in train_ids}\n\nreferences_test = {video_id: references[video_id] for video_id in test_ids}\nhypotheses_test = {video_id: hypotheses[video_id] for video_id in test_ids}\n\n# # Evaluate on train set\n# caption_metrics_train = evaluate_caption_metrics(references_train, hypotheses_train)\n# print(\"Train Set Caption Quality Metrics:\")\n# for metric, score in caption_metrics_train.items():\n#     print(f\"{metric}: {score:.4f}\")\n\n# Evaluate on test set\ncaption_metrics_test = evaluate_caption_metrics(references_test, hypotheses_test)\nprint(\"\\nTest Set Caption Quality Metrics:\")\nfor metric, score in caption_metrics_test.items():\n    print(f\"{metric}: {score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluate the Model\n2. Proposal Quality Evaluation","metadata":{}},{"cell_type":"code","source":"# # Example proposals and ground truths\n# proposals = [\n#     (\"video_1\", 10.0, 20.0, 0.95),  # (video_id, start, end, confidence)\n#     (\"video_1\", 25.0, 35.0, 0.90),\n#     (\"video_2\", 15.0, 25.0, 0.85)\n# ]\n\n# ground_truths = [\n#     (\"video_1\", 10.0, 20.0),  # (video_id, start, end)\n#     (\"video_1\", 30.0, 40.0),\n#     (\"video_2\", 15.0, 25.0)\n# ]\n\n# iou_thresholds = [0.3, 0.5, 0.7, 0.9]\n# k_values = [1, 5, 10]\n\n# proposal_metrics = compute_recall_precision(proposals, ground_truths, iou_thresholds, k_values)\n# print(\"Proposal Quality Metrics:\")\n# for metric, score in proposal_metrics.items():\n#     print(f\"{metric}: {score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install nltk rouge-score\n# !pip install --upgrade nltk\n\n# from nltk.translate.bleu_score import sentence_bleu\n# from nltk.translate.meteor_score import meteor_score\n# from rouge_score import rouge_scorer\n# import numpy as np\n# from collections import defaultdict\n\n# # Function to compute evaluation metrics\n# def evaluate_captions(generated_captions, ground_truth_captions):\n#     \"\"\"\n#     Evaluate the generated captions against ground truth captions using BLEU, METEOR, and ROUGE.\n\n#     Args:\n#         generated_captions (dict): Dictionary with video_id as keys and generated captions as values.\n#         ground_truth_captions (dict): Dictionary with video_id as keys and ground truth captions as values.\n\n#     Returns:\n#         dict: Dictionary containing average BLEU, METEOR, and ROUGE scores.\n#     \"\"\"\n#     bleu_scores = []\n#     meteor_scores = []\n#     rouge_scores = defaultdict(list)\n\n#     rouge_scorer_tool = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n\n#     for video_id in generated_captions:\n#         if video_id not in ground_truth_captions:\n#             print(f\"Warning: Ground truth caption missing for video ID {video_id}\")\n#             continue\n\n#         generated = generated_captions[video_id]\n#         references = [ground_truth_captions[video_id]]\n\n#         # Compute BLEU score\n#         bleu_scores.append(sentence_bleu(references, generated.split()))\n\n#         # Compute METEOR score\n#         meteor_scores.append(meteor_score(references, generated))\n\n#         # Compute ROUGE scores\n#         rouge_result = rouge_scorer_tool.score(\" \".join(references), generated)\n#         for metric in rouge_result:\n#             rouge_scores[metric].append(rouge_result[metric].fmeasure)\n\n#     # Compute average scores\n#     avg_bleu = np.mean(bleu_scores)\n#     avg_meteor = np.mean(meteor_scores)\n#     avg_rouge = {metric: np.mean(scores) for metric, scores in rouge_scores.items()}\n\n#     return {\n#         \"BLEU\": avg_bleu,\n#         \"METEOR\": avg_meteor,\n#         \"ROUGE\": avg_rouge,\n#     }\n\n# # Example usage with testing dataset\n# # Assuming `generated_captions_test` is a dict of video_id -> generated caption\n# # And `ground_truth_captions_test` is a dict of video_id -> ground truth caption(s)\n# print(\"Evaluating captions on the testing dataset...\")\n# metrics = evaluate_captions(generated_captions_test, ground_truth_captions_test)\n\n# print(f\"Evaluation Results:\")\n# print(f\"  BLEU Score: {metrics['BLEU']:.4f}\")\n# print(f\"  METEOR Score: {metrics['METEOR']:.4f}\")\n# print(f\"  ROUGE Scores:\")\n# for metric, score in metrics[\"ROUGE\"].items():\n#     print(f\"    {metric}: {score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing on TEST Data","metadata":{}},{"cell_type":"code","source":"\n# def generate_caption_for_proposal(model, proposal_feature, tokenizer, max_seq_length, temperature=1.0, top_k=5):\n#     model.eval()  # Set model to evaluation mode\n    \n#     # Convert proposal_feature to PyTorch tensor\n#     proposal_feature = torch.tensor(proposal_feature, dtype=torch.float32)\n    \n#     # Ensure proposal_feature has three dimensions (batch_size, seq_length, feature_dim)\n#     if len(proposal_feature.shape) == 1:  # If shape is (feature_dim,)\n#         proposal_feature = proposal_feature.unsqueeze(0).unsqueeze(0)  # Add seq_length and batch dimensions\n#     elif len(proposal_feature.shape) == 2:  # If shape is (seq_length, feature_dim)\n#         proposal_feature = proposal_feature.unsqueeze(0)  # Add batch dimension\n    \n#     # Move to device\n#     proposal_feature = proposal_feature.to(device)\n\n#     generated_caption = []\n#     input_ids = torch.tensor([tokenizer.cls_token_id], dtype=torch.long).unsqueeze(0).to(device)  # Start with CLS token\n    \n#     for step in range(max_seq_length):\n#         with torch.no_grad():\n#             logits = model(proposal_feature, input_ids)  # Forward pass through the decoder\n#             # print(f\"Step {step + 1}: Logits shape: {logits.shape}\")  # Debug logits\n            \n#             logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n#             probabilities = torch.softmax(logits, dim=-1)\n            \n#             # Sampling from Top-K probabilities\n#             top_k_probs, top_k_indices = torch.topk(probabilities, k=top_k, dim=-1)\n#             # print(f\"Top-K Tokens: {top_k_indices.squeeze()}, Top-K Probabilities: {top_k_probs.squeeze()}\")  # Debug\n            \n#             next_token_index = top_k_probs.squeeze().multinomial(1).item()  # Sample from probabilities\n#             next_token = top_k_indices.squeeze()[next_token_index]  # Map index back to token ID\n            \n#             # print(f\"Generated Token: {next_token}\")  # Debug generated token\n            \n#             generated_caption.append(next_token.item())\n#             input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)\n\n#             if next_token.item() == tokenizer.sep_token_id:  # Stop if SEP token is generated\n#                 break\n\n#     # Decode generated tokens to text\n#     caption = tokenizer.decode(generated_caption, skip_special_tokens=True)\n#     # print(f\"Generated Caption: {caption}\")  # Debug final caption\n#     return caption\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### splitting the Data to train and test data","metadata":{}},{"cell_type":"code","source":"# Initialize a dictionary to store video-wise captions\nvideo_captions = {}\n\nfrom sklearn.model_selection import train_test_split\n\n# Split filtered_proposals_train into train and test\nvideo_ids = list(filtered_proposals_train.keys())  # Extract all video IDs\ntrain_ids, test_ids = train_test_split(video_ids, test_size=0.5, random_state=42)\n\n# Create train and test splits\nfiltered_proposals_train_split = {video_id: filtered_proposals_train[video_id] for video_id in train_ids}\nfiltered_proposals_test_split = {video_id: filtered_proposals_train[video_id] for video_id in test_ids}\n\n# Function to process proposals and generate video-wise captions\ndef process_and_generate_captions(filtered_proposals, refined_features):\n    video_captions = {}\n\n    for video_id, proposal_list in filtered_proposals.items():\n        if video_id not in refined_features:\n            continue\n\n        video_features = refined_features[video_id]\n        num_frames = video_features.shape[0]\n        merged_caption = []\n\n        for idx, proposal in enumerate(proposal_list):\n            # Extract start and end times\n            start_time = proposal.get(\"start\", 0.0)\n            end_time = proposal.get(\"end\", 1.0)\n\n            # Scale to match frame indices\n            start_idx = int(start_time * num_frames)\n            end_idx = int(end_time * num_frames)\n\n            # Check for valid indices\n            if start_idx < 0 or end_idx > num_frames or start_idx >= end_idx:\n                continue\n\n            # Extract proposal feature and apply mean pooling\n            proposal_feature = video_features[start_idx:end_idx]\n            if proposal_feature.shape[0] == 0:\n                continue\n            proposal_feature = np.mean(proposal_feature, axis=0)\n\n            # Generate caption\n            caption = generate_caption_for_proposal(caption_model, proposal_feature, tokenizer, max_seq_length)\n            merged_caption.append(caption)\n\n        # Combine all proposal captions into a single caption for the video\n        video_captions[video_id] = \" \".join(merged_caption)\n\n    return video_captions\n\n# Process train and test splits\n# print(\"Generating captions for train set...\")\n# video_captions_train = process_and_generate_captions(filtered_proposals_train_split, refined_features_train)\n\nprint(\"\\nGenerating captions for test set...\")\nvideo_captions_test = process_and_generate_captions(filtered_proposals_test_split, refined_features_train)\n\n# # Print results\n# print(\"\\nTrain Set Video-wise Captions:\")\n# for video_id, caption in video_captions_train.items():\n#     print(f\"Video ID: {video_id}, Caption: {caption}\")\n\nprint(\"\\nTest Set Video-wise Captions:\")\nfor video_id, caption in video_captions_test.items():\n    print(f\"Video ID: {video_id}, Caption: {caption}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Debug feature values for a proposal\nproposal_feature_debug = np.mean(refined_features_train[sample_video_id][:10], axis=0)\nprint(f\"Debug proposal feature: {proposal_feature_debug[:10]}\")  # Check if values are reasonable\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Ensure random_state is consistent\ntrain_ids, test_ids = train_test_split(video_ids, test_size=0.2, random_state=42)\n\nfiltered_proposals_train_split = {video_id: filtered_proposals_train[video_id] for video_id in train_ids}\nfiltered_proposals_test_split = {video_id: filtered_proposals_train[video_id] for video_id in test_ids}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption_for_proposal(model, proposal_feature, tokenizer, max_seq_length):\n    \"\"\"\n    Generate a caption for a given proposal feature using the caption model.\n\n    Args:\n        model: Pre-trained caption generation model.\n        proposal_feature: Feature vector for the proposal.\n        tokenizer: Tokenizer for decoding token IDs to text.\n        max_seq_length: Maximum length of the generated caption.\n\n    Returns:\n        str: Generated caption.\n    \"\"\"\n    model.eval()\n    proposal_feature = torch.tensor(proposal_feature, dtype=torch.float32).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_ids = torch.tensor([tokenizer.cls_token_id], dtype=torch.long).unsqueeze(0).to(proposal_feature.device)\n\n    generated_caption = []\n    for _ in range(max_seq_length):\n        with torch.no_grad():\n            logits = model(proposal_feature, input_ids)  # Forward pass\n            next_token = logits[:, -1, :].argmax(dim=-1)  # Get the token with the highest probability\n            generated_caption.append(next_token.item())\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n            if next_token.item() == tokenizer.sep_token_id:\n                break\n\n    return tokenizer.decode(generated_caption, skip_special_tokens=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n\n# Function to generate a caption for a proposal feature\ndef generate_caption_for_proposal(model, proposal_feature, tokenizer, max_seq_length):\n    \"\"\"\n    Generate a caption for a given proposal feature using the caption model.\n\n    Args:\n        model: Pre-trained caption generation model.\n        proposal_feature: Feature vector for the proposal.\n        tokenizer: Tokenizer for decoding token IDs to text.\n        max_seq_length: Maximum length of the generated caption.\n\n    Returns:\n        str: Generated caption.\n    \"\"\"\n    model.eval()\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Ensure proposal_feature is 3D: [1, seq_length, feature_dim]\n    if len(proposal_feature.shape) == 1:\n        proposal_feature = proposal_feature[np.newaxis, np.newaxis, :]  # [feature_dim] -> [1, 1, feature_dim]\n    elif len(proposal_feature.shape) == 2:\n        proposal_feature = proposal_feature[np.newaxis, :, :]  # [seq_length, feature_dim] -> [1, seq_length, feature_dim]\n    proposal_feature = torch.tensor(proposal_feature, dtype=torch.float32).to(device)\n\n    input_ids = torch.tensor([tokenizer.cls_token_id], dtype=torch.long).unsqueeze(0).to(device)\n\n    generated_caption = []\n    for _ in range(max_seq_length):\n        with torch.no_grad():\n            logits = model(proposal_feature, input_ids)\n            top_k = 5  # Adjust for diversity\n            probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n            top_k_probs, top_k_indices = torch.topk(probabilities, k=top_k, dim=-1)\n            next_token = top_k_indices.squeeze(0)[torch.multinomial(top_k_probs.squeeze(0), 1).item()]\n            \n            # Ensure next_token is reshaped properly for concatenation\n            next_token = next_token.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1]\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n            \n            generated_caption.append(next_token.item())\n            if next_token.item() == tokenizer.sep_token_id:\n                break\n\n    return tokenizer.decode(generated_caption, skip_special_tokens=True)\n\n\n# Function to process proposals and generate video-wise captions\ndef process_and_generate_captions(filtered_proposals, refined_features, caption_model, tokenizer, max_seq_length):\n    video_captions = {}\n\n    for video_id, proposal_list in filtered_proposals.items():\n        if video_id not in refined_features:\n            continue\n\n        video_features = refined_features[video_id]\n        num_frames = video_features.shape[0]\n        merged_caption = []\n\n        for idx, proposal in enumerate(proposal_list):\n            # Extract start and end times\n            start_time = proposal.get(\"start\", 0.0)\n            end_time = proposal.get(\"end\", 1.0)\n\n            # Scale to match frame indices\n            start_idx = int(start_time * num_frames)\n            end_idx = int(end_time * num_frames)\n\n            # Check for valid indices\n            if start_idx < 0 or end_idx > num_frames or start_idx >= end_idx:\n                continue\n\n            # Extract proposal feature and apply mean pooling\n            proposal_feature = video_features[start_idx:end_idx]\n            if proposal_feature.shape[0] == 0:\n                continue\n            proposal_feature = np.mean(proposal_feature, axis=0)\n\n            # Generate caption\n            caption = generate_caption_for_proposal(caption_model, proposal_feature, tokenizer, max_seq_length)\n            merged_caption.append(caption)\n\n        # Combine all proposal captions into a single caption for the video\n        video_captions[video_id] = \" \".join(merged_caption)\n\n    return video_captions\n\n\n# Splitting filtered_proposals_train into train and test\nvideo_ids = list(filtered_proposals_train.keys())  # Extract all video IDs\ntrain_ids, test_ids = train_test_split(video_ids, test_size=0.5, random_state=42)\n\n# Create train and test splits\nfiltered_proposals_train_split = {video_id: filtered_proposals_train[video_id] for video_id in train_ids}\nfiltered_proposals_test_split = {video_id: filtered_proposals_train[video_id] for video_id in test_ids}\n\n# Process train and test splits\nprint(\"\\nGenerating captions for test set...\")\nvideo_captions_test = process_and_generate_captions(\n    filtered_proposals_test_split,\n    refined_features_train,\n    caption_model,\n    tokenizer,\n    max_seq_length\n)\n\n# Print results\nprint(\"\\nTest Set Video-wise Captions:\")\nfor video_id, caption in video_captions_test.items():\n    print(f\"Video ID: {video_id}, Caption: {caption}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the video ID you want to evaluate\nspecific_video_id = \"RUnYJCdTLiY\"  # Replace with the specific video ID\n\n# Ensure the video ID exists in both references and hypotheses\nif specific_video_id in references and specific_video_id in hypotheses:\n    specific_reference = {specific_video_id: references[specific_video_id]}\n    specific_hypothesis = {specific_video_id: hypotheses[specific_video_id]}\n\n    # Evaluate the specific video\n    specific_caption_metrics = evaluate_caption_metrics(specific_reference, specific_hypothesis)\n    \n    # Display results for the specific video\n    print(f\"\\nCaption Quality Metrics for Video ID: {specific_video_id}\")\n    for metric, score in specific_caption_metrics.items():\n        print(f\"{metric}: {score:.4f}\")\nelse:\n    print(f\"Video ID {specific_video_id} not found in references or hypotheses.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the video ID you want to evaluate\nspecific_video_id = \"sbkVPzkVY0M\"  # Replace with the specific video ID\n\n# Ensure the video ID exists in both references and hypotheses\nif specific_video_id in references and specific_video_id in hypotheses:\n    specific_reference = {specific_video_id: references[specific_video_id]}\n    specific_hypothesis = {specific_video_id: hypotheses[specific_video_id]}\n\n    # Evaluate the specific video\n    specific_caption_metrics = evaluate_caption_metrics(specific_reference, specific_hypothesis)\n    \n    # Display results for the specific video\n    print(f\"\\nCaption Quality Metrics for Video ID: {specific_video_id}\")\n    for metric, score in specific_caption_metrics.items():\n        print(f\"{metric}: {score:.4f}\")\nelse:\n    print(f\"Video ID {specific_video_id} not found in references or hypotheses.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import torch\n\n# # Load the annotations\n# annotations_path = \"/kaggle/input/annotations/youcookii_annotations_trainval/youcookii_annotations_trainval.json\"  # Replace with your path\n# with open(annotations_path, \"r\") as f:\n#     annotations = json.load(f)[\"database\"]\n\n\n# def generate_ground_truth_batch(video_ids, annotations, num_chunks, chunk_size, video_duration):\n#     \"\"\"\n#     Generate ground truth for a batch of videos.\n\n#     Args:\n#         video_ids: List of video IDs in the batch.\n#         annotations: Dictionary of video annotations.\n#         num_chunks: Number of temporal chunks.\n#         chunk_size: Number of frames per chunk.\n#         video_duration: Duration of the video in seconds.\n\n#     Returns:\n#         gt_start_batch: Tensor of shape [batch_size, num_chunks] for start scores.\n#         gt_end_batch: Tensor of shape [batch_size, num_chunks] for end scores.\n#     \"\"\"\n#     gt_start_batch = []\n#     gt_end_batch = []\n\n#     stride = video_duration / num_chunks  # Duration per chunk\n#     for video_id in video_ids:\n#         video_info = annotations[video_id]\n#         start_times = [ann[\"segment\"][0] for ann in video_info[\"annotations\"]]\n#         end_times = [ann[\"segment\"][1] for ann in video_info[\"annotations\"]]\n        \n#         # Generate ground truth for the current video\n#         gt_start = torch.zeros(num_chunks)\n#         gt_end = torch.zeros(num_chunks)\n#         for start, end in zip(start_times, end_times):\n#             start_idx = int(start / stride)\n#             end_idx = int(end / stride)\n#             if start_idx < num_chunks:\n#                 gt_start[start_idx] = 1\n#             if end_idx < num_chunks:\n#                 gt_end[end_idx] = 1\n\n#         gt_start_batch.append(gt_start)\n#         gt_end_batch.append(gt_end)\n\n#     return torch.stack(gt_start_batch), torch.stack(gt_end_batch)\n\n\n# # Extract durations and ground truth from annotations\n# # def fetch_video_annotations(annotations, video_id):\n# #     \"\"\"\n# #     Fetch the duration, start, and end times from annotations for a given video ID.\n\n# #     Args:\n# #         annotations: Loaded JSON annotations.\n# #         video_id: Video ID for which to fetch data.\n\n# #     Returns:\n# #         video_duration: Duration of the video.\n# #         start_times: List of start times for segments.\n# #         end_times: List of end times for segments.\n# #     \"\"\"\n# #     video_info = annotations[\"database\"][video_id]\n# #     video_duration = video_info[\"duration\"]\n# #     start_times = [segment[\"segment\"][0] for segment in video_info[\"annotations\"]]\n# #     end_times = [segment[\"segment\"][1] for segment in video_info[\"annotations\"]]\n# #     return video_duration, start_times, end_times\n\n# def fetch_video_annotations(annotations, video_id):\n#     \"\"\"\n#     Fetch the duration, start, and end times from annotations for a given video ID.\n\n#     Args:\n#         annotations: Loaded JSON annotations.\n#         video_id: Video ID for which to fetch data.\n\n#     Returns:\n#         video_duration: Duration of the video.\n#         start_times: List of start times for segments.\n#         end_times: List of end times for segments.\n#     \"\"\"\n#     # Assuming `annotations` directly contains video IDs as keys\n#     if video_id not in annotations:\n#         raise KeyError(f\"Video ID '{video_id}' not found in annotations.\")\n\n#     video_info = annotations[video_id]\n#     video_duration = video_info[\"duration\"]\n#     start_times = [segment[\"segment\"][0] for segment in video_info[\"annotations\"]]\n#     end_times = [segment[\"segment\"][1] for segment in video_info[\"annotations\"]]\n\n#     return video_duration, start_times, end_times\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Training loop\n# for epoch in range(num_epochs):\n#     epoch_loss_train = 0.0\n#     model.train()\n\n#     for video_ids, features in proposal_dataloader_train:\n#         features = features.to(\"cuda\") if torch.cuda.is_available() else features\n#         model = model.to(\"cuda\") if torch.cuda.is_available() else model\n\n\n#         # Fetch duration and ground truth start/end times from annotations\n#         video_id = video_ids[0]  # Assuming batch size of 1 for simplicity\n#         video_duration, start_times, end_times = fetch_video_annotations(annotations, video_id)\n#         print(\"id:\", video_id , \"::\", \"Duration:\", video_duration)\n\n        \n#         # Generate ground truth for the batch\n#         num_chunks = features.shape[1]\n#         # video_duration = 10.0  # Replace with actual duration for each video\n#         gt_start, gt_end = generate_ground_truth_batch(\n#             video_ids, annotations, num_chunks, chunk_size, video_duration\n#         )\n#         gt_start = gt_start.to(\"cuda\") if torch.cuda.is_available() else gt_start\n#         gt_end = gt_end.to(\"cuda\") if torch.cuda.is_available() else gt_end\n\n#         # Forward pass\n#         start_scores, end_scores = model(features)\n\n#         # Compute loss\n#         loss_start = criterion(start_scores, gt_start)\n#         loss_end = criterion(end_scores, gt_end)\n#         loss = loss_start + loss_end\n\n#         # Backward pass and optimization\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         epoch_loss_train += loss.item()\n\n#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss_train:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}